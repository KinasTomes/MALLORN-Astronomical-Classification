{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":13720337,"sourceType":"datasetVersion","datasetId":8729261},{"sourceId":14080758,"sourceType":"datasetVersion","datasetId":8964107}],"dockerImageVersionId":31234,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip -q install -U \"tabpfn == 2.2.1\"\n!pip -q install pytorch-tabnet\n\n# ============================================================\n# CELL 1: IMPORTS & CONFIGURATION\n# ============================================================\n\nimport os, random, gc, warnings\nwarnings.filterwarnings(\"ignore\")\n\nSEED = 42\nos.environ[\"PYTHONHASHSEED\"] = str(SEED)\nos.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"2\"\nos.environ[\"TF_DETERMINISTIC_OPS\"] = \"1\"\n\nrandom.seed(SEED)\n\nimport numpy as np\nnp.random.seed(SEED)\n\nimport pandas as pd\nimport joblib\n\nimport tensorflow as tf\ntf.random.set_seed(SEED)\ntry:\n    tf.config.experimental.enable_op_determinism()\nexcept Exception:\n    pass\n\nfrom tensorflow import keras\nfrom tensorflow.keras.layers import (\n    Input, Dense, GRU, Bidirectional, Layer,\n    Concatenate, Dropout, SpatialDropout1D, BatchNormalization,\n    GaussianNoise, Masking\n)\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\n\nfrom pathlib import Path\nfrom tqdm import tqdm\n\nfrom sklearn.model_selection import StratifiedKFold, train_test_split\nfrom sklearn.metrics import f1_score, roc_auc_score, classification_report\nfrom sklearn.preprocessing import RobustScaler, StandardScaler\nfrom sklearn.feature_selection import SelectKBest, f_classif\nfrom scipy.optimize import minimize_scalar\n\nimport lightgbm as lgb\nimport xgboost as xgb\nfrom catboost import CatBoostClassifier\nfrom tabpfn import TabPFNClassifier\nfrom sklearn.ensemble import HistGradientBoostingClassifier\nfrom pytorch_tabnet.tab_model import TabNetClassifier\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader\nimport optuna\nfrom optuna.samplers import TPESampler\n\nkeras.backend.clear_session()\ngc.collect()\n\nprint(\"TF:\", tf.__version__, \"| PyTorch:\", torch.__version__, \"| SEED =\", SEED)\n\nDATA_PATH = Path(\"/kaggle/input/project/mallorn-astronomical-classification-challenge\")\n\nBANDS = [\"u\", \"g\", \"r\", \"i\", \"z\", \"y\"]\nBAND_MAP = {b: i for i, b in enumerate(BANDS)}\n\nMAX_SEQ_LEN = 300\nN_FEATURES_PER_STEP = 4\n\nMODEL_DIR = \"saved_models\"\nos.makedirs(MODEL_DIR, exist_ok=True)\n\nGBM_MODEL_DIR = \"/kaggle/working/models\"\nos.makedirs(GBM_MODEL_DIR, exist_ok=True)\n\nFEAT_TRAIN_PKL = \"/kaggle/input/2d-gp-features/kaggle/working/cache/train_features_2dgp_gpy.pkl\"\nFEAT_TEST_PKL  = \"/kaggle/input/2d-gp-features/kaggle/working/cache/test_features_2dgp_gpy.pkl\"\n\nN_FOLDS = 5\nskf = StratifiedKFold(n_splits=N_FOLDS, shuffle=True, random_state=SEED)\n\n# SelectKBest configuration (cho cả NN meta và GBM)\nUSE_SELECTKBEST = False   # Set False to disable feature selection\nK_BEST_RATIO = 0.8       # Keep 80% of features (or use absolute number if < 1)\nK_BEST_MIN = 200         # Minimum number of features to keep\nK_BEST_MAX = None        # Maximum number of features (None = no limit)\n\n# Device for PyTorch models\nDEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\nprint(f\"PyTorch models will use device: {DEVICE}\")\n\nprint(\"Config done!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-21T16:42:17.171839Z","iopub.execute_input":"2025-12-21T16:42:17.172873Z","iopub.status.idle":"2025-12-21T16:42:27.087960Z","shell.execute_reply.started":"2025-12-21T16:42:17.172839Z","shell.execute_reply":"2025-12-21T16:42:27.086592Z"}},"outputs":[{"name":"stdout","text":"TF: 2.19.0 | PyTorch: 2.8.0+cu126 | SEED = 42\nPyTorch models will use device: cpu\nConfig done!\n","output_type":"stream"}],"execution_count":26},{"cell_type":"code","source":"# ============================================================\n# CELL 2: LOAD DATA\n# ============================================================\n\nprint(\"\\n--- Loading Data ---\")\n\ntrain_log = pd.read_csv(DATA_PATH / \"train_log.csv\")\ntest_log  = pd.read_csv(DATA_PATH / \"test_log.csv\")\n\ndef load_lc(split, kind):\n    return pd.read_csv(DATA_PATH / split / f\"{kind}_full_lightcurves.csv\")\n\ntrain_lc = pd.concat([load_lc(s, \"train\") for s in train_log[\"split\"].unique()], ignore_index=True)\ntest_lc  = pd.concat([load_lc(s, \"test\")  for s in test_log[\"split\"].unique()],  ignore_index=True)\n\ntrain_feat = pd.read_pickle(FEAT_TRAIN_PKL)\ntest_feat  = pd.read_pickle(FEAT_TEST_PKL)\n\ny = train_log[\"target\"].values.astype(np.int32)\n\nprint(f\"Train LC: {train_lc.shape}, Test LC: {test_lc.shape}\")\nprint(f\"Train objects: {len(train_log)}, Test objects: {len(test_log)}\")\nprint(f\"TDE count: {train_log['target'].sum()}, TDE ratio: {train_log['target'].mean()*100:.2f}%\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-21T16:43:09.049809Z","iopub.execute_input":"2025-12-21T16:43:09.050200Z","iopub.status.idle":"2025-12-21T16:43:10.437155Z","shell.execute_reply.started":"2025-12-21T16:43:09.050169Z","shell.execute_reply":"2025-12-21T16:43:10.436192Z"}},"outputs":[{"name":"stdout","text":"\n--- Loading Data ---\nTrain LC: (479384, 5), Test LC: (1145125, 5)\nTrain objects: 3043, Test objects: 7135\nTDE count: 148, TDE ratio: 4.86%\n","output_type":"stream"}],"execution_count":27},{"cell_type":"code","source":"# ============================================================\n# CELL 8: TRAIN LIGHTGBM (WITH BEST-FOLD SUBMISSION)\n# ============================================================\nimport numpy as np\nimport pandas as pd\nimport joblib\nfrom itertools import combinations\nfrom scipy.stats import rankdata\nfrom scipy.optimize import minimize_scalar\n\nprint(\"\\n\" + \"=\"*60)\nprint(\"TRAINING LIGHTGBM\")\nprint(\"=\"*60)\n\nlgb_params = {\n    \"objective\": \"binary\",\n    \"metric\": \"binary_logloss\",\n    \"boosting_type\": \"gbdt\",\n    \"num_leaves\": 31,\n    \"max_depth\": 7,\n    \"learning_rate\": 0.05,\n    \"feature_fraction\": 0.8,\n    \"bagging_fraction\": 0.8,\n    \"bagging_freq\": 5,\n    \"min_child_samples\": 20,\n    \"scale_pos_weight\": scale_pos_weight,\n    \"verbose\": -1,\n    \"seed\": SEED,\n    \"n_jobs\": -1,\n    \"feature_fraction_seed\": SEED,\n    \"bagging_seed\": SEED,\n    \"data_random_seed\": SEED,\n}\n\nlgb_oof = np.zeros(len(y))\nlgb_test = np.zeros(len(X_test))\nlgb_models = []\nlgb_fold_scores = []\nlgb_fold_thresholds = []\nlgb_test_folds = []\n\nfor fold, (train_idx, val_idx) in enumerate(skf.split(X, y), 1):\n    print(f\"\\n--- Fold {fold}/{N_FOLDS} ---\")\n\n    X_tr, X_val = X[train_idx], X[val_idx]\n    y_tr, y_val = y[train_idx], y[val_idx]\n\n    train_data = lgb.Dataset(X_tr, label=y_tr)\n    val_data = lgb.Dataset(X_val, label=y_val, reference=train_data)\n\n    model = lgb.train(\n        lgb_params,\n        train_data,\n        num_boost_round=2000,\n        valid_sets=[val_data],\n        callbacks=[lgb.early_stopping(100, verbose=False)],\n    )\n\n    lgb_models.append(model)\n    fold_oof = model.predict(X_val)\n    lgb_oof[val_idx] = fold_oof\n\n    fold_test = model.predict(X_test)\n    lgb_test += fold_test / N_FOLDS\n    lgb_test_folds.append(fold_test)\n\n    best_th, best_f1 = 0.5, 0\n    for th in np.arange(0.1, 0.9, 0.01):\n        f1 = f1_score(y_val, (fold_oof >= th).astype(int))\n        if f1 > best_f1:\n            best_f1, best_th = f1, th\n\n    lgb_fold_scores.append(best_f1)\n    lgb_fold_thresholds.append(best_th)\n    print(f\"Fold {fold}: F1={best_f1:.4f} at threshold={best_th:.2f}\")\n\ndef neg_f1(th):\n    return -f1_score(y, (lgb_oof >= th).astype(int))\n\nresult = minimize_scalar(neg_f1, bounds=(0.1, 0.9), method=\"bounded\")\nlgb_best_th = result.x\nlgb_best_f1 = -result.fun\nlgb_auc = roc_auc_score(y, lgb_oof)\n\nprint(f\"\\nLightGBM CV F1: {np.mean(lgb_fold_scores):.4f} ± {np.std(lgb_fold_scores):.4f}\")\nprint(f\"LightGBM Global F1: {lgb_best_f1:.4f} at threshold={lgb_best_th:.2f}\")\nprint(f\"LightGBM ROC-AUC: {lgb_auc:.4f}\")\n\n# Submission từ fold có F1 cao nhất\nbest_lgb_fold = int(np.argmax(lgb_fold_scores))\nbest_lgb_th = float(lgb_fold_thresholds[best_lgb_fold])\nbest_lgb_test = np.asarray(lgb_test_folds[best_lgb_fold], dtype=float)\n\nlgb_single_preds = (best_lgb_test >= best_lgb_th).astype(int)\nsub_lgb_single = pd.DataFrame({\n    \"object_id\": test_log[\"object_id\"],\n    \"target\": lgb_single_preds\n})\nsub_lgb_single.to_csv(\"submission_single_fold_LGB.csv\", index=False)\nprint(f\"\\nSaved single-fold LGB submission (best fold #{best_lgb_fold+1}) -> submission_single_fold_LGB.csv\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-21T16:43:14.159919Z","iopub.execute_input":"2025-12-21T16:43:14.160900Z","iopub.status.idle":"2025-12-21T16:43:27.080746Z","shell.execute_reply.started":"2025-12-21T16:43:14.160858Z","shell.execute_reply":"2025-12-21T16:43:27.079260Z"}},"outputs":[{"name":"stdout","text":"\n============================================================\nTRAINING LIGHTGBM\n============================================================\n\n--- Fold 1/5 ---\nFold 1: F1=0.5263 at threshold=0.19\n\n--- Fold 2/5 ---\nFold 2: F1=0.6250 at threshold=0.30\n\n--- Fold 3/5 ---\nFold 3: F1=0.5067 at threshold=0.12\n\n--- Fold 4/5 ---\nFold 4: F1=0.6667 at threshold=0.34\n\n--- Fold 5/5 ---\nFold 5: F1=0.5758 at threshold=0.35\n\nLightGBM CV F1: 0.5801 ± 0.0597\nLightGBM Global F1: 0.5631 at threshold=0.34\nLightGBM ROC-AUC: 0.9425\n\nSaved single-fold LGB submission (best fold #4) -> submission_single_fold_LGB.csv\n","output_type":"stream"}],"execution_count":28},{"cell_type":"code","source":"# ============================================================\n# CELL 9: FEATURE STATISTICS ANALYSIS\n# ============================================================\n\nprint(\"\\n\" + \"=\"*70)\nprint(\"ANALYSIS 1: FEATURE STATISTICS\")\nprint(\"=\"*70)\n\n# Analyze original features before scaling\nprint(\"\\n1. NaN/Inf Analysis:\")\nnan_counts = pd.DataFrame({\n    'feature': feature_names,\n    'nan_count': [np.isnan(X_raw[:, i]).sum() for i in range(X_raw.shape[1])],\n    'inf_count': [np.isinf(X_raw[:, i]).sum() for i in range(X_raw.shape[1])],\n    'zero_count': [(X_raw[:, i] == 0).sum() for i in range(X_raw.shape[1])],\n})\n\nnan_counts['nan_pct'] = (nan_counts['nan_count'] / len(X_raw) * 100).round(2)\nnan_counts['zero_pct'] = (nan_counts['zero_count'] / len(X_raw) * 100).round(2)\n\nhigh_nan = nan_counts[nan_counts['nan_pct'] > 50].sort_values('nan_pct', ascending=False)\nprint(f\"\\n  Total features: {len(nan_counts)}\")\nprint(f\"  Features with >50% NaN: {len(high_nan)}\")\nif len(high_nan) > 0:\n    print(f\"  Top 10 features with most NaN:\")\n    print(high_nan.head(10)[['feature', 'nan_pct', 'zero_pct']].to_string(index=False))\n\nprint(\"\\n2. Feature Scale Analysis:\")\nfeature_stats = pd.DataFrame({\n    'feature': feature_names,\n    'mean': [np.nanmean(X_raw[:, i]) for i in range(X_raw.shape[1])],\n    'std': [np.nanstd(X_raw[:, i]) for i in range(X_raw.shape[1])],\n    'min': [np.nanmin(X_raw[:, i]) for i in range(X_raw.shape[1])],\n    'max': [np.nanmax(X_raw[:, i]) for i in range(X_raw.shape[1])],\n    'q25': [np.nanpercentile(X_raw[:, i], 25) for i in range(X_raw.shape[1])],\n    'q75': [np.nanpercentile(X_raw[:, i], 75) for i in range(X_raw.shape[1])],\n})\n\n# Remove NaN/inf for analysis\nfeature_stats = feature_stats.replace([np.inf, -np.inf], np.nan)\n\nprint(f\"  Mean std across features: {feature_stats['std'].mean():.4f}\")\nprint(f\"  Max std: {feature_stats['std'].max():.4f}\")\nprint(f\"  Min std: {feature_stats['std'].min():.4f}\")\n\n# Features with extreme scales\nextreme_std = feature_stats[feature_stats['std'] > 1000].sort_values('std', ascending=False)\nif len(extreme_std) > 0:\n    print(f\"\\n  Features with VERY HIGH std (>1000): {len(extreme_std)}\")\n    print(extreme_std.head(10)[['feature', 'std', 'min', 'max']].to_string(index=False))\n\nzero_var = feature_stats[feature_stats['std'] < 1e-6]\nif len(zero_var) > 0:\n    print(f\"\\n  Features with ZERO/NEAR-ZERO variance (<1e-6): {len(zero_var)}\")\n    print(zero_var[['feature', 'std']].head(20).to_string(index=False))\n\nprint(\"\\n3. Feature Distribution Skewness:\")\nfrom scipy.stats import skew\nfeature_stats['skewness'] = [skew(X_raw[:, i]) if np.nanstd(X_raw[:, i]) > 1e-6 else 0 \n                             for i in range(X_raw.shape[1])]\nhigh_skew = feature_stats[feature_stats['skewness'].abs() > 5].sort_values('skewness', key=abs, ascending=False)\nif len(high_skew) > 0:\n    print(f\"  Features with HIGH skewness (|skew| > 5): {len(high_skew)}\")\n    print(high_skew.head(10)[['feature', 'skewness', 'mean', 'std']].to_string(index=False))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-21T16:46:05.195734Z","iopub.execute_input":"2025-12-21T16:46:05.196107Z","iopub.status.idle":"2025-12-21T16:46:05.621934Z","shell.execute_reply.started":"2025-12-21T16:46:05.196079Z","shell.execute_reply":"2025-12-21T16:46:05.620882Z"}},"outputs":[{"name":"stdout","text":"\n======================================================================\nANALYSIS 1: FEATURE STATISTICS\n======================================================================\n\n1. NaN/Inf Analysis:\n\n  Total features: 308\n  Features with >50% NaN: 0\n\n2. Feature Scale Analysis:\n  Mean std across features: 3663.5524\n  Max std: 1097813.4700\n  Min std: 0.0000\n\n  Features with VERY HIGH std (>1000): 7\n               feature          std           min          max\n         gp2d_ls_ratio 1.097813e+06  6.153001e-08 4.299914e+07\nu_gp2d_integrated_flux 2.955390e+03 -3.252791e+04 8.786704e+04\ng_gp2d_integrated_flux 2.624058e+03 -2.264464e+04 6.584307e+04\nr_gp2d_integrated_flux 2.447970e+03 -1.457523e+04 4.652942e+04\ni_gp2d_integrated_flux 2.395894e+03 -1.254920e+04 4.101660e+04\ny_gp2d_integrated_flux 2.349733e+03 -1.171562e+04 3.981636e+04\nz_gp2d_integrated_flux 2.348133e+03 -1.210663e+04 4.057562e+04\n\n  Features with ZERO/NEAR-ZERO variance (<1e-6): 5\n     feature  std\n       Z_err  0.0\ncount_snr_-3  0.0\ncount_snr_-5  0.0\n frac_snr_-3  0.0\n frac_snr_-5  0.0\n\n3. Feature Distribution Skewness:\n  Features with HIGH skewness (|skew| > 5): 119\n                  feature  skewness         mean          std\ny_gp2d_rise_decline_ratio 47.172489     3.224239 4.102087e+01\n   gp2d_length_scale_wave 40.187550    68.874448 8.140546e+02\nu_gp2d_rise_decline_ratio 33.811164     2.679148 2.739702e+01\n            gp2d_ls_ratio 33.410622 36393.851094 1.097813e+06\n   gp2d_length_scale_time 32.430631     0.150614 1.153628e+00\n      gp2d_peak_ratio_z_i 30.970659     0.990096 7.777007e-01\n         r_gp2d_rise_rate 27.513628     0.004800 1.518006e-02\n         i_gp2d_rise_rate 26.551997     0.005117 1.581873e-02\ng_gp2d_rise_decline_ratio 26.366216     2.462979 1.683355e+01\ni_gp2d_rise_decline_ratio 25.959848     2.138108 1.061683e+01\n","output_type":"stream"}],"execution_count":29},{"cell_type":"markdown","source":"=> Apply Select Features","metadata":{}},{"cell_type":"code","source":"# ============================================================\n# CELL 7: PREPARE FEATURES WITH SelectKBest (FOR GBM MODELS)\n# ============================================================\nUSE_SELECTKBEST = True\nprint(\"\\n\" + \"=\"*60)\nprint(\"LOADING DATA (REUSED IN-MEMORY) FOR GBM\")\nprint(\"=\"*60)\nK_BEST_RATIO = 0.8\nX_df = train_feat.copy()\nX_test_df = test_feat.copy()\n\nprint(f\"Train features: {X_df.shape}\")\nprint(f\"Test features: {X_test_df.shape}\")\nprint(f\"TDE count: {y.sum()}, TDE ratio: {y.mean()*100:.2f}%\")\n\nprint(\"\\n--- Preparing GBM Features ---\")\n\nif \"object_id\" in X_df.columns:\n    X_df = X_df.drop(columns=[\"object_id\"])\nif \"object_id\" in X_test_df.columns:\n    X_test_df = X_test_df.drop(columns=[\"object_id\"])\n\n# Deterministic order\ncommon_cols = sorted(set(X_df.columns) & set(X_test_df.columns))\nX_df = X_df[common_cols]\nX_test_df = X_test_df[common_cols]\n\nX_df = X_df.replace([np.inf, -np.inf], np.nan).fillna(0)\nX_test_df = X_test_df.replace([np.inf, -np.inf], np.nan).fillna(0)\n\nfeature_names = X_df.columns.tolist()\nX_raw = X_df.values\nX_test_raw = X_test_df.values\n\nn_features_original = X_raw.shape[1]\nprint(f\"Original number of GBM features: {n_features_original}\")\n\nfeature_selector = None\nselected_feature_names = feature_names\n\nos.makedirs(GBM_MODEL_DIR, exist_ok=True)\n\nif USE_SELECTKBEST and n_features_original > K_BEST_MIN:\n    # Calculate k_best\n    if K_BEST_RATIO < 1.0:\n        k_best = int(n_features_original * K_BEST_RATIO)\n    else:\n        k_best = int(K_BEST_RATIO)\n    \n    # Apply min/max constraints\n    k_best = max(K_BEST_MIN, k_best)\n    if K_BEST_MAX is not None:\n        k_best = min(K_BEST_MAX, k_best)\n    k_best = min(k_best, n_features_original)\n    \n    print(f\"\\nApplying SelectKBest (GBM): {n_features_original} -> {k_best} features\")\n    print(f\"  (ratio={K_BEST_RATIO}, min={K_BEST_MIN}, max={K_BEST_MAX})\")\n    \n    # Fit SelectKBest on full training data\n    feature_selector = SelectKBest(score_func=f_classif, k=k_best)\n    X = feature_selector.fit_transform(X_raw, y)\n    X_test = feature_selector.transform(X_test_raw)\n    \n    # Get selected feature names\n    selected_mask = feature_selector.get_support()\n    selected_feature_names = [feature_names[i] for i in range(len(feature_names)) if selected_mask[i]]\n    \n    print(f\"Selected {len(selected_feature_names)} features for GBM\")\n    \n    # Save selector\n    joblib.dump(feature_selector, f\"{GBM_MODEL_DIR}/gbm_feature_selector.pkl\")\n    joblib.dump(selected_feature_names, f\"{GBM_MODEL_DIR}/gbm_selected_feature_names.pkl\")\n    print(f\"✅ Saved GBM feature selector and selected feature names\")\n    \nelse:\n    print(f\"\\nSkipping SelectKBest for GBM (using all {n_features_original} features)\")\n    X = X_raw\n    X_test = X_test_raw\n    \n    # Save all feature names\n    joblib.dump(selected_feature_names, f\"{GBM_MODEL_DIR}/gbm_all_feature_names.pkl\")\n\nprint(f\"\\nFinal GBM feature count: {X.shape[1]}\")\nprint(f\"Final GBM train shape: {X.shape}, Final GBM test shape: {X_test.shape}\")\n\nscale_pos_weight = (y == 0).sum() / (y == 1).sum()\nprint(f\"Scale pos weight: {scale_pos_weight:.2f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-21T16:46:10.020269Z","iopub.execute_input":"2025-12-21T16:46:10.020621Z","iopub.status.idle":"2025-12-21T16:46:10.117378Z","shell.execute_reply.started":"2025-12-21T16:46:10.020593Z","shell.execute_reply":"2025-12-21T16:46:10.116429Z"}},"outputs":[{"name":"stdout","text":"\n============================================================\nLOADING DATA (REUSED IN-MEMORY) FOR GBM\n============================================================\nTrain features: (3043, 309)\nTest features: (7135, 309)\nTDE count: 148, TDE ratio: 4.86%\n\n--- Preparing GBM Features ---\nOriginal number of GBM features: 308\n\nApplying SelectKBest (GBM): 308 -> 246 features\n  (ratio=0.8, min=200, max=None)\nSelected 246 features for GBM\n✅ Saved GBM feature selector and selected feature names\n\nFinal GBM feature count: 246\nFinal GBM train shape: (3043, 246), Final GBM test shape: (7135, 246)\nScale pos weight: 19.56\n","output_type":"stream"}],"execution_count":30},{"cell_type":"code","source":"# ============================================================\n# CELL 8: TRAIN LIGHTGBM (WITH BEST-FOLD SUBMISSION)\n# ============================================================\nimport numpy as np\nimport pandas as pd\nimport joblib\nfrom itertools import combinations\nfrom scipy.stats import rankdata\nfrom scipy.optimize import minimize_scalar\n\nprint(\"\\n\" + \"=\"*60)\nprint(\"TRAINING LIGHTGBM\")\nprint(\"=\"*60)\n\nlgb_params = {\n    \"objective\": \"binary\",\n    \"metric\": \"binary_logloss\",\n    \"boosting_type\": \"gbdt\",\n    \"num_leaves\": 31,\n    \"max_depth\": 7,\n    \"learning_rate\": 0.05,\n    \"feature_fraction\": 0.8,\n    \"bagging_fraction\": 0.8,\n    \"bagging_freq\": 5,\n    \"min_child_samples\": 20,\n    \"scale_pos_weight\": scale_pos_weight,\n    \"verbose\": -1,\n    \"seed\": SEED,\n    \"n_jobs\": -1,\n    \"feature_fraction_seed\": SEED,\n    \"bagging_seed\": SEED,\n    \"data_random_seed\": SEED,\n}\n\nlgb_oof = np.zeros(len(y))\nlgb_test = np.zeros(len(X_test))\nlgb_models = []\nlgb_fold_scores = []\nlgb_fold_thresholds = []\nlgb_test_folds = []\n\nfor fold, (train_idx, val_idx) in enumerate(skf.split(X, y), 1):\n    print(f\"\\n--- Fold {fold}/{N_FOLDS} ---\")\n\n    X_tr, X_val = X[train_idx], X[val_idx]\n    y_tr, y_val = y[train_idx], y[val_idx]\n\n    train_data = lgb.Dataset(X_tr, label=y_tr)\n    val_data = lgb.Dataset(X_val, label=y_val, reference=train_data)\n\n    model = lgb.train(\n        lgb_params,\n        train_data,\n        num_boost_round=2000,\n        valid_sets=[val_data],\n        callbacks=[lgb.early_stopping(100, verbose=False)],\n    )\n\n    lgb_models.append(model)\n    fold_oof = model.predict(X_val)\n    lgb_oof[val_idx] = fold_oof\n\n    fold_test = model.predict(X_test)\n    lgb_test += fold_test / N_FOLDS\n    lgb_test_folds.append(fold_test)\n\n    best_th, best_f1 = 0.5, 0\n    for th in np.arange(0.1, 0.9, 0.01):\n        f1 = f1_score(y_val, (fold_oof >= th).astype(int))\n        if f1 > best_f1:\n            best_f1, best_th = f1, th\n\n    lgb_fold_scores.append(best_f1)\n    lgb_fold_thresholds.append(best_th)\n    print(f\"Fold {fold}: F1={best_f1:.4f} at threshold={best_th:.2f}\")\n\ndef neg_f1(th):\n    return -f1_score(y, (lgb_oof >= th).astype(int))\n\nresult = minimize_scalar(neg_f1, bounds=(0.1, 0.9), method=\"bounded\")\nlgb_best_th = result.x\nlgb_best_f1 = -result.fun\nlgb_auc = roc_auc_score(y, lgb_oof)\n\nprint(f\"\\nLightGBM CV F1: {np.mean(lgb_fold_scores):.4f} ± {np.std(lgb_fold_scores):.4f}\")\nprint(f\"LightGBM Global F1: {lgb_best_f1:.4f} at threshold={lgb_best_th:.2f}\")\nprint(f\"LightGBM ROC-AUC: {lgb_auc:.4f}\")\n\n# Submission từ fold có F1 cao nhất\nbest_lgb_fold = int(np.argmax(lgb_fold_scores))\nbest_lgb_th = float(lgb_fold_thresholds[best_lgb_fold])\nbest_lgb_test = np.asarray(lgb_test_folds[best_lgb_fold], dtype=float)\n\nlgb_single_preds = (best_lgb_test >= best_lgb_th).astype(int)\nsub_lgb_single = pd.DataFrame({\n    \"object_id\": test_log[\"object_id\"],\n    \"target\": lgb_single_preds\n})\nsub_lgb_single.to_csv(\"submission_single_fold_LGB.csv\", index=False)\nprint(f\"\\nSaved single-fold LGB submission (best fold #{best_lgb_fold+1}) -> submission_single_fold_LGB.csv\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-21T16:46:11.662910Z","iopub.execute_input":"2025-12-21T16:46:11.663786Z","iopub.status.idle":"2025-12-21T16:46:24.439817Z","shell.execute_reply.started":"2025-12-21T16:46:11.663752Z","shell.execute_reply":"2025-12-21T16:46:24.438746Z"}},"outputs":[{"name":"stdout","text":"\n============================================================\nTRAINING LIGHTGBM\n============================================================\n\n--- Fold 1/5 ---\nFold 1: F1=0.5263 at threshold=0.19\n\n--- Fold 2/5 ---\nFold 2: F1=0.6250 at threshold=0.30\n\n--- Fold 3/5 ---\nFold 3: F1=0.5067 at threshold=0.12\n\n--- Fold 4/5 ---\nFold 4: F1=0.6667 at threshold=0.34\n\n--- Fold 5/5 ---\nFold 5: F1=0.5758 at threshold=0.35\n\nLightGBM CV F1: 0.5801 ± 0.0597\nLightGBM Global F1: 0.5631 at threshold=0.34\nLightGBM ROC-AUC: 0.9425\n\nSaved single-fold LGB submission (best fold #4) -> submission_single_fold_LGB.csv\n","output_type":"stream"}],"execution_count":31},{"cell_type":"code","source":"# ============================================================\n# CELL 18: SHAP ANALYSIS FOR WEAKNESS DETECTION\n# ============================================================\n\nprint(\"\\n\" + \"=\"*70)\nprint(\"ANALYSIS: SHAP VALUES FOR FEATURE IMPORTANCE\")\nprint(\"=\"*70)\n\ntry:\n    import shap\n    print(\"✅ SHAP library available\\n\")\nexcept ImportError:\n    print(\"❌ Installing SHAP...\")\n    !pip install -q shap\n    import shap\n    print(\"✅ SHAP installed\\n\")\n\n# Use a sample for SHAP (tính toán nhanh hơn)\n# Hoặc dùng toàn bộ data nếu có đủ thời gian\nSAMPLE_SIZE = min(10000, len(X))  # Sample size for SHAP calculation\n\nprint(f\"Using sample size: {SAMPLE_SIZE} for SHAP calculation\")\nprint(\"(For full dataset, increase SAMPLE_SIZE or use tree explainer)\\n\")\n\n# Sample data\nsample_idx = np.random.choice(len(X), size=SAMPLE_SIZE, replace=False)\nX_sample = X[sample_idx]\ny_sample = y[sample_idx]\n\n# Use one of the trained models (best fold)\nbest_model_idx = np.argmax(lgb_fold_scores)\nshap_model = lgb_models[best_model_idx]\n\nprint(f\"Using model from fold {best_model_idx + 1} (best F1: {lgb_fold_scores[best_model_idx]:.4f})\\n\")\n\n# Calculate SHAP values\nprint(\"Calculating SHAP values...\")\nexplainer = shap.TreeExplainer(shap_model)\nshap_values = explainer.shap_values(X_sample)\n\n# shap_values is a list: [shap_values_class_0, shap_values_class_1]\n# For binary classification, we'll use class 1 (TDE)\nshap_values_class1 = shap_values[1] if isinstance(shap_values, list) else shap_values\n\nprint(f\"✅ SHAP values calculated: shape {shap_values_class1.shape}\\n\")\n\n# ============================================================\n# 1. GLOBAL FEATURE IMPORTANCE (SHAP)\n# ============================================================\nprint(\"=\"*70)\nprint(\"1. GLOBAL FEATURE IMPORTANCE (SHAP)\")\nprint(\"=\"*70)\n\n# Mean absolute SHAP values = global importance\nshap_importance = np.abs(shap_values_class1).mean(axis=0)\nshap_importance_df = pd.DataFrame({\n    'feature': feature_names[:len(shap_importance)],  # Ensure same length\n    'shap_importance': shap_importance,\n}).sort_values('shap_importance', ascending=False)\n\nprint(\"\\nTop 30 Features by SHAP Importance:\")\nprint(shap_importance_df.head(30).to_string(index=False))\n\nprint(\"\\nBottom 20 Features by SHAP Importance (potentially useless):\")\nprint(shap_importance_df.tail(20).to_string(index=False))\n\n# Compare SHAP importance vs LGB gain importance\nif 'feature_importance_df' in locals():\n    print(\"\\n\" + \"-\"*70)\n    print(\"SHAP vs LGB Gain Importance Comparison:\")\n    \n    comparison_df = shap_importance_df.merge(\n        feature_importance_df[['feature', 'importance']],\n        on='feature',\n        how='left'\n    )\n    comparison_df['importance_norm'] = (\n        comparison_df['importance'] - comparison_df['importance'].min()\n    ) / (comparison_df['importance'].max() - comparison_df['importance'].min() + 1e-10)\n    comparison_df['shap_norm'] = (\n        comparison_df['shap_importance'] - comparison_df['shap_importance'].min()\n    ) / (comparison_df['shap_importance'].max() - comparison_df['shap_importance'].min() + 1e-10)\n    \n    corr_shap_lgb = np.corrcoef(\n        comparison_df['shap_norm'], \n        comparison_df['importance_norm']\n    )[0, 1]\n    \n    print(f\"   Correlation: {corr_shap_lgb:.4f}\")\n    \n    if corr_shap_lgb < 0.5:\n        print(f\"   ⚠️  LOW correlation! Different features are important from SHAP perspective!\")\n    \n    # Features important in SHAP but not in LGB\n    comparison_df['importance_diff'] = comparison_df['shap_norm'] - comparison_df['importance_norm']\n    underused_shap = comparison_df.nlargest(20, 'importance_diff')\n    \n    print(f\"\\n   Top 20 Features MORE Important in SHAP than LGB Gain:\")\n    print(underused_shap[['feature', 'shap_importance', 'importance', 'importance_diff']].to_string(index=False))\n    print(f\"   → These features may be underutilized by current model!\")\n\n# ============================================================\n# 2. FEATURE IMPACT BY CLASS\n# ============================================================\nprint(\"\\n\" + \"=\"*70)\nprint(\"2. FEATURE IMPACT BY CLASS (TDE vs Non-TDE)\")\nprint(\"=\"*70)\n\n# Separate SHAP values by actual class\ntde_shap = shap_values_class1[y_sample == 1]\nnontde_shap = shap_values_class1[y_sample == 0]\n\nprint(f\"\\nTDE samples: {len(tde_shap)}, Non-TDE samples: {len(nontde_shap)}\")\n\ntde_mean_shap = np.abs(tde_shap).mean(axis=0)\nnontde_mean_shap = np.abs(nontde_shap).mean(axis=0)\n\nclass_impact_df = pd.DataFrame({\n    'feature': feature_names[:len(tde_mean_shap)],\n    'shap_TDE': tde_mean_shap,\n    'shap_NonTDE': nontde_mean_shap,\n    'shap_diff': tde_mean_shap - nontde_mean_shap,\n}).sort_values('shap_diff', ascending=False)\n\nprint(\"\\nTop 20 Features MORE Important for TDE predictions:\")\nprint(class_impact_df.head(20)[['feature', 'shap_TDE', 'shap_NonTDE', 'shap_diff']].to_string(index=False))\n\nprint(\"\\nTop 20 Features MORE Important for Non-TDE predictions:\")\nprint(class_impact_df.tail(20)[['feature', 'shap_TDE', 'shap_NonTDE', 'shap_diff']].to_string(index=False))\n\n# Features with very different importance between classes\nclass_specific = class_impact_df[np.abs(class_impact_df['shap_diff']) > 0.01].sort_values('shap_diff', key=abs, ascending=False)\nprint(f\"\\n   Found {len(class_specific)} features with CLASS-SPECIFIC importance:\")\nprint(class_specific.head(15)[['feature', 'shap_TDE', 'shap_NonTDE', 'shap_diff']].to_string(index=False))\nprint(f\"   → These features behave differently for TDE vs Non-TDE\")\n\n# ============================================================\n# 3. MISCLASSIFICATION ANALYSIS WITH SHAP\n# ============================================================\nprint(\"\\n\" + \"=\"*70)\nprint(\"3. MISCLASSIFICATION ANALYSIS WITH SHAP\")\nprint(\"=\"*70)\n\n# Get predictions for sample\nsample_preds = shap_model.predict(X_sample)\nsample_preds_binary = (sample_preds >= lgb_fold_thresholds[best_model_idx]).astype(int)\nsample_y = y_sample\n\n# Find misclassifications\nfp_mask = (sample_preds_binary == 1) & (sample_y == 0)\nfn_mask = (sample_preds_binary == 0) & (sample_y == 1)\ntp_mask = (sample_preds_binary == 1) & (sample_y == 1)\ntn_mask = (sample_preds_binary == 0) & (sample_y == 0)\n\nprint(f\"\\nFP (False Positives): {fp_mask.sum()}\")\nprint(f\"FN (False Negatives): {fn_mask.sum()}\")\nprint(f\"TP (True Positives): {tp_mask.sum()}\")\nprint(f\"TN (True Negatives): {tn_mask.sum()}\")\n\nif fp_mask.sum() > 0:\n    fp_shap = shap_values_class1[fp_mask]\n    tn_shap = shap_values_class1[tn_mask][:min(fp_mask.sum(), tn_mask.sum())]\n    \n    fp_mean_shap = np.abs(fp_shap).mean(axis=0)\n    tn_mean_shap = np.abs(tn_shap).mean(axis=0)\n    \n    fp_analysis = pd.DataFrame({\n        'feature': feature_names[:len(fp_mean_shap)],\n        'fp_shap': fp_mean_shap,\n        'tn_shap': tn_mean_shap,\n        'diff': fp_mean_shap - tn_mean_shap,\n    }).sort_values('diff', ascending=False)\n    \n    print(\"\\n   Top 15 Features causing False Positives (confusing Non-TDE as TDE):\")\n    print(fp_analysis.head(15)[['feature', 'fp_shap', 'tn_shap', 'diff']].to_string(index=False))\n\nif fn_mask.sum() > 0:\n    fn_shap = shap_values_class1[fn_mask]\n    tp_shap = shap_values_class1[tp_mask][:min(fn_mask.sum(), tp_mask.sum())]\n    \n    fn_mean_shap = np.abs(fn_shap).mean(axis=0)\n    tp_mean_shap = np.abs(tp_shap).mean(axis=0)\n    \n    fn_analysis = pd.DataFrame({\n        'feature': feature_names[:len(fn_mean_shap)],\n        'fn_shap': fn_mean_shap,\n        'tp_shap': tp_mean_shap,\n        'diff': fn_mean_shap - tp_mean_shap,\n    }).sort_values('diff', ascending=False)\n    \n    print(\"\\n   Top 15 Features causing False Negatives (missing TDE):\")\n    print(fn_analysis.head(15)[['feature', 'fn_shap', 'tp_shap', 'diff']].to_string(index=False))\n\n# ============================================================\n# 4. FEATURE VALUES THAT CONFUSE MODEL\n# ============================================================\nprint(\"\\n\" + \"=\"*70)\nprint(\"4. FEATURE VALUES THAT CAUSE CONFUSION\")\nprint(\"=\"*70)\n\n# For misclassified samples, check if SHAP values are unusually high/low\nif fp_mask.sum() > 0 or fn_mask.sum() > 0:\n    misclassified_shap = shap_values_class1[fp_mask | fn_mask]\n    correctly_classified_shap = shap_values_class1[tp_mask | tn_mask]\n    \n    if len(misclassified_shap) > 0 and len(correctly_classified_shap) > 0:\n        misclassified_mean = misclassified_shap.mean(axis=0)\n        correct_mean = correctly_classified_shap.mean(axis=0)\n        \n        confusion_df = pd.DataFrame({\n            'feature': feature_names[:len(misclassified_mean)],\n            'misclassified_shap': misclassified_mean,\n            'correct_shap': correct_mean,\n            'shap_diff': misclassified_mean - correct_mean,\n        }).sort_values('shap_diff', key=abs, ascending=False)\n        \n        print(\"\\n   Features with DIFFERENT SHAP values for misclassified vs correctly classified:\")\n        print(confusion_df.head(20)[['feature', 'misclassified_shap', 'correct_shap', 'shap_diff']].to_string(index=False))\n\nprint(\"\\n\" + \"=\"*70)\nprint(\"KEY INSIGHTS FROM SHAP\")\nprint(\"=\"*70)\n\ninsights = []\n\nif 'underused_shap' in locals() and len(underused_shap) > 10:\n    insights.append(f\"❌ {len(underused_shap)} features are important in SHAP but underused by LGB gain importance\")\n    insights.append(\"   → Model may benefit from feature engineering or different algorithm\")\n\nif len(class_specific) > 10:\n    insights.append(f\"⚠️  {len(class_specific)} features have class-specific importance\")\n    insights.append(\"   → Consider class-specific feature engineering or weighted features\")\n\nif fp_mask.sum() > 5 or fn_mask.sum() > 5:\n    insights.append(f\"⚠️  Misclassifications found: {fp_mask.sum()} FP, {fn_mask.sum()} FN\")\n    insights.append(\"   → Check feature values and interactions for these cases\")\n\nif len(insights) == 0:\n    print(\"\\n✅ SHAP analysis shows consistent feature importance\")\nelse:\n    print(\"\\n\")\n    for i, insight in enumerate(insights, 1):\n        print(f\"{i}. {insight}\")\n\nprint(\"\\n\" + \"=\"*70)\n\n# Save SHAP importance for later use\nshap_importance_df.to_csv(f\"{GBM_MODEL_DIR}/shap_importance.csv\", index=False)\nprint(f\"\\n✅ Saved SHAP importance to {GBM_MODEL_DIR}/shap_importance.csv\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-21T16:50:36.009282Z","iopub.execute_input":"2025-12-21T16:50:36.009614Z","iopub.status.idle":"2025-12-21T16:50:38.237913Z","shell.execute_reply.started":"2025-12-21T16:50:36.009579Z","shell.execute_reply":"2025-12-21T16:50:38.236936Z"}},"outputs":[{"name":"stdout","text":"\n======================================================================\nANALYSIS: SHAP VALUES FOR FEATURE IMPORTANCE\n======================================================================\n✅ SHAP library available\n\nUsing sample size: 3043 for SHAP calculation\n(For full dataset, increase SAMPLE_SIZE or use tree explainer)\n\nUsing model from fold 4 (best F1: 0.6667)\n\nCalculating SHAP values...\n✅ SHAP values calculated: shape (3043, 246)\n\n======================================================================\n1. GLOBAL FEATURE IMPORTANCE (SHAP)\n======================================================================\n\nTop 30 Features by SHAP Importance:\n                feature  shap_importance\ni_gp2d_peaks_pos_frac_2         0.488558\n    i_gp2d_time_fwd_0.5         0.420239\n              i_snr_max         0.355115\n    i_gp2d_decline_rate         0.312741\n           flux_std_all         0.203499\n         flux_range_all         0.187759\n           flux_max_all         0.181696\n    r_gp2d_time_bwd_0.8         0.167534\n      count_max_fall_30         0.159222\n             r_flux_std         0.157541\n             i_flux_min         0.151830\n    g_gp2d_time_fwd_0.5         0.148489\n  i_gp2d_positive_width         0.137879\n               g_pct_25         0.131938\n       g_gp2d_peak_time         0.131741\n    i_gp2d_time_fwd_0.8         0.130865\n                      Z         0.129377\n            g_gp2d_fwhm         0.125259\n               g_pct_50         0.124717\n             r_flux_max         0.123002\n               g_pct_75         0.122575\n    r_gp2d_time_bwd_0.2         0.102583\n             i_snr_mean         0.098154\n               i_pct_75         0.091270\n        g_gp2d_mean_std         0.085867\ngp2d_spectral_evolution         0.080210\n        u_gp2d_abs_diff         0.079229\n             g_flux_max         0.079205\n       r_gp2d_peak_time         0.074918\n    gp2d_blue_red_ratio         0.074556\n\nBottom 20 Features by SHAP Importance (potentially useless):\n                  feature  shap_importance\n               snr_median         0.000488\n   r_gp2d_peaks_pos_count         0.000467\n    i_gp2d_negative_width         0.000399\ni_gp2d_rise_decline_ratio         0.000366\n              y_flux_mean         0.000366\n              u_gp2d_fwhm         0.000200\n         gp2d_peak_dt_i_z         0.000183\n                r_snr_max         0.000158\n              frac_snr_10         0.000142\n           u_gp2d_min_std         0.000137\n         gp2d_peak_dt_g_z         0.000091\n           g_gp2d_max_std         0.000062\n     gp2d_kernel_variance         0.000000\n           gp2d_color_r_i         0.000000\n              frac_snr_20         0.000000\n    r_gp2d_positive_width         0.000000\n  i_gp2d_peaks_neg_frac_2         0.000000\n  r_gp2d_peaks_pos_frac_3         0.000000\n   r_gp2d_integrated_flux         0.000000\n           r_gp2d_max_std         0.000000\n\n======================================================================\n2. FEATURE IMPACT BY CLASS (TDE vs Non-TDE)\n======================================================================\n\nTDE samples: 148, Non-TDE samples: 2895\n\nTop 20 Features MORE Important for TDE predictions:\n                feature  shap_TDE  shap_NonTDE  shap_diff\n    i_gp2d_decline_rate  0.537583     0.301246   0.236337\n         flux_range_all  0.371728     0.178354   0.193374\n              i_snr_max  0.507508     0.347324   0.160184\n    i_gp2d_time_fwd_0.5  0.559091     0.413140   0.145951\n    i_gp2d_time_fwd_0.8  0.254815     0.124528   0.130288\n    r_gp2d_time_bwd_0.8  0.281284     0.161719   0.119566\n             r_flux_std  0.267975     0.151895   0.116080\n                      Z  0.224463     0.124516   0.099946\n               i_pct_75  0.178554     0.086808   0.091746\n               g_pct_75  0.209463     0.118133   0.091330\n            g_gp2d_fwhm  0.211939     0.120828   0.091111\ni_gp2d_peaks_pos_frac_2  0.572791     0.484252   0.088539\n               g_pct_25  0.209010     0.127998   0.081012\n    g_gp2d_time_fwd_0.5  0.212020     0.145241   0.066779\n      count_max_fall_30  0.219141     0.156159   0.062982\n               g_pct_50  0.170777     0.122362   0.048415\n    r_gp2d_time_fwd_0.8  0.096256     0.048029   0.048227\n           flux_max_all  0.225585     0.179453   0.046132\n              color_g_i  0.086595     0.040992   0.045603\n    r_gp2d_time_bwd_0.2  0.143708     0.100481   0.043227\n\nTop 20 Features MORE Important for Non-TDE predictions:\n                  feature  shap_TDE  shap_NonTDE     shap_diff\n              frac_snr_20  0.000000     0.000000  0.000000e+00\n           r_gp2d_max_std  0.000000     0.000000  0.000000e+00\n    r_gp2d_positive_width  0.000000     0.000000  0.000000e+00\n                r_snr_max  0.000157     0.000158 -9.893809e-07\n           g_gp2d_max_std  0.000042     0.000063 -2.087259e-05\n      u_gp2d_time_bwd_0.8  0.001000     0.001024 -2.440747e-05\n         gp2d_peak_dt_g_z  0.000042     0.000093 -5.169133e-05\n         gp2d_peak_dt_i_z  0.000131     0.000186 -5.480836e-05\n      i_gp2d_time_bwd_0.5  0.000620     0.000714 -9.417008e-05\n   r_gp2d_peaks_pos_count  0.000339     0.000474 -1.355052e-04\ni_gp2d_rise_decline_ratio  0.000236     0.000372 -1.362020e-04\n   gp2d_length_scale_time  0.001406     0.001675 -2.686243e-04\n   g_gp2d_integrated_flux  0.001111     0.001490 -3.793518e-04\n   i_gp2d_integrated_flux  0.001743     0.002208 -4.648110e-04\n               i_flux_max  0.052488     0.053016 -5.274592e-04\n         gp2d_peak_dt_u_g  0.002855     0.003402 -5.473413e-04\n              r_total_snr  0.006275     0.006953 -6.785851e-04\n                  g_n_obs  0.005105     0.006493 -1.387560e-03\n   u_gp2d_integrated_flux  0.001018     0.002472 -1.454315e-03\n      g_gp2d_time_fwd_0.8  0.012686     0.015529 -2.842486e-03\n\n   Found 78 features with CLASS-SPECIFIC importance:\n                feature  shap_TDE  shap_NonTDE  shap_diff\n    i_gp2d_decline_rate  0.537583     0.301246   0.236337\n         flux_range_all  0.371728     0.178354   0.193374\n              i_snr_max  0.507508     0.347324   0.160184\n    i_gp2d_time_fwd_0.5  0.559091     0.413140   0.145951\n    i_gp2d_time_fwd_0.8  0.254815     0.124528   0.130288\n    r_gp2d_time_bwd_0.8  0.281284     0.161719   0.119566\n             r_flux_std  0.267975     0.151895   0.116080\n                      Z  0.224463     0.124516   0.099946\n               i_pct_75  0.178554     0.086808   0.091746\n               g_pct_75  0.209463     0.118133   0.091330\n            g_gp2d_fwhm  0.211939     0.120828   0.091111\ni_gp2d_peaks_pos_frac_2  0.572791     0.484252   0.088539\n               g_pct_25  0.209010     0.127998   0.081012\n    g_gp2d_time_fwd_0.5  0.212020     0.145241   0.066779\n      count_max_fall_30  0.219141     0.156159   0.062982\n   → These features behave differently for TDE vs Non-TDE\n\n======================================================================\n3. MISCLASSIFICATION ANALYSIS WITH SHAP\n======================================================================\n\nFP (False Positives): 11\nFN (False Negatives): 9\nTP (True Positives): 139\nTN (True Negatives): 2884\n\n   Top 15 Features causing False Positives (confusing Non-TDE as TDE):\n                feature  fp_shap  tn_shap     diff\n    i_gp2d_time_fwd_0.5 0.539579 0.258852 0.280727\n    i_gp2d_decline_rate 0.555728 0.316977 0.238751\n         flux_range_all 0.380163 0.203913 0.176250\n        u_gp2d_abs_diff 0.210015 0.043557 0.166458\n    r_gp2d_time_bwd_0.8 0.305047 0.168730 0.136317\n              i_snr_max 0.435016 0.327889 0.107126\n               g_pct_75 0.169764 0.084790 0.084974\n            g_gp2d_fwhm 0.218408 0.139516 0.078892\n               g_pct_25 0.184852 0.116457 0.068395\n             r_flux_std 0.239806 0.172236 0.067570\n               i_pct_75 0.134324 0.068482 0.065842\n    r_gp2d_time_bwd_0.2 0.164509 0.104857 0.059652\n              color_g_i 0.108476 0.052843 0.055633\n    i_gp2d_time_fwd_0.8 0.197077 0.148051 0.049026\nu_gp2d_peaks_neg_frac_2 0.088906 0.040142 0.048764\n\n   Top 15 Features causing False Negatives (missing TDE):\n                feature  fn_shap  tp_shap     diff\n        u_gp2d_abs_diff 0.134089 0.072137 0.061952\n             i_snr_mean 0.096496 0.052401 0.044095\n                      Z 0.266210 0.223926 0.042284\nu_gp2d_peaks_pos_frac_2 0.067853 0.039202 0.028650\n       gp2d_peak_dt_u_r 0.054604 0.028558 0.026046\n           count_snr_-5 0.054051 0.028082 0.025969\n               g_pct_50 0.210392 0.187972 0.022420\n  y_gp2d_negative_width 0.061859 0.039505 0.022354\n       r_gp2d_peak_flux 0.056660 0.035059 0.021601\n            i_flux_mean 0.087951 0.066797 0.021154\n                i_n_obs 0.036472 0.015861 0.020612\n    r_gp2d_time_bwd_0.8 0.257927 0.238736 0.019191\ng_gp2d_peaks_neg_frac_2 0.056291 0.037600 0.018691\n         y_gp2d_min_std 0.055839 0.038099 0.017740\n       g_gp2d_peak_flux 0.046894 0.030306 0.016587\n\n======================================================================\n4. FEATURE VALUES THAT CAUSE CONFUSION\n======================================================================\n\n   Features with DIFFERENT SHAP values for misclassified vs correctly classified:\n                feature  misclassified_shap  correct_shap  shap_diff\ni_gp2d_peaks_pos_frac_2            0.521968     -0.026776   0.548744\n    i_gp2d_time_fwd_0.5            0.505722     -0.004082   0.509804\n    i_gp2d_decline_rate            0.378190     -0.077304   0.455494\n              i_snr_max            0.409852     -0.014491   0.424344\n         flux_range_all            0.315242      0.024871   0.290371\n    i_gp2d_time_fwd_0.8            0.176599      0.022771   0.153828\n  i_gp2d_positive_width            0.149826     -0.000881   0.150706\n      count_max_fall_30            0.167974      0.017855   0.150119\n             r_flux_std            0.136990     -0.005976   0.142965\n             i_flux_min            0.134683     -0.005685   0.140369\n    r_gp2d_time_bwd_0.8            0.130656      0.003248   0.127408\n            g_gp2d_fwhm            0.114245      0.000129   0.114116\n    r_gp2d_time_bwd_0.2            0.106953      0.004217   0.102736\n             r_flux_max            0.085840     -0.010536   0.096376\n               g_pct_75            0.095063      0.004399   0.090665\n           flux_std_all            0.088935      0.001964   0.086971\n           flux_max_all            0.076584     -0.006720   0.083304\n        g_gp2d_mean_std            0.086104      0.003674   0.082430\ngp2d_spectral_evolution            0.083860      0.001492   0.082369\n             g_flux_max            0.085046      0.003431   0.081615\n\n======================================================================\nKEY INSIGHTS FROM SHAP\n======================================================================\n\n\n1. ⚠️  78 features have class-specific importance\n2.    → Consider class-specific feature engineering or weighted features\n3. ⚠️  Misclassifications found: 11 FP, 9 FN\n4.    → Check feature values and interactions for these cases\n\n======================================================================\n\n✅ Saved SHAP importance to /kaggle/working/models/shap_importance.csv\n","output_type":"stream"}],"execution_count":32},{"cell_type":"code","source":"# ============================================================\n# CELL 19: SHAP-BASED FEATURE OPTIMIZATION\n# ============================================================\n\nprint(\"\\n\" + \"=\"*70)\nprint(\"SHAP-BASED FEATURE OPTIMIZATION\")\nprint(\"=\"*70)\n\n# Load SHAP importance if not in memory\nimport os\nif 'shap_importance_df' not in locals():\n    if os.path.exists(f\"{GBM_MODEL_DIR}/shap_importance.csv\"):\n        shap_importance_df = pd.read_csv(f\"{GBM_MODEL_DIR}/shap_importance.csv\")\n        print(\"✅ Loaded SHAP importance from file\")\n    else:\n        print(\"❌ SHAP importance not found. Run CELL 18 first!\")\n        raise FileNotFoundError(\"Please run CELL 18 (SHAP Analysis) first\")\n\nprint(f\"\\nTotal features (after SelectKBest): {len(shap_importance_df)}\")\n\n# Get original feature data (before SelectKBest transformation)\n# We need to work with original X_df, X_test_df\nX_df_orig = train_feat.copy()\nX_test_df_orig = test_feat.copy()\n\nif \"object_id\" in X_df_orig.columns:\n    X_df_orig = X_df_orig.drop(columns=[\"object_id\"])\nif \"object_id\" in X_test_df_orig.columns:\n    X_test_df_orig = X_test_df_orig.drop(columns=[\"object_id\"])\n\ncommon_cols_orig = sorted(set(X_df_orig.columns) & set(X_test_df_orig.columns))\nX_df_orig = X_df_orig[common_cols_orig]\nX_test_df_orig = X_test_df_orig[common_cols_orig]\n\nX_df_orig = X_df_orig.replace([np.inf, -np.inf], np.nan).fillna(0)\nX_test_df_orig = X_test_df_orig.replace([np.inf, -np.inf], np.nan).fillna(0)\n\n# ============================================================\n# STEP 1: REMOVE USELESS FEATURES (SHAP importance < threshold)\n# ============================================================\nprint(\"\\n\" + \"-\"*70)\nprint(\"STEP 1: REMOVE USELESS FEATURES\")\nprint(\"-\"*70)\n\n# Threshold: remove features with SHAP importance < 0.001\nSHAP_THRESHOLD = 0.001\n\nuseless_features = shap_importance_df[\n    shap_importance_df['shap_importance'] < SHAP_THRESHOLD\n]['feature'].tolist()\n\nprint(f\"Features to remove (SHAP < {SHAP_THRESHOLD}): {len(useless_features)}\")\nif len(useless_features) > 0:\n    print(f\"  Examples: {useless_features[:10]}\")\n\n# Remove from original datasets\nX_df_cleaned = X_df_orig.drop(columns=[f for f in useless_features if f in X_df_orig.columns], errors='ignore')\nX_test_df_cleaned = X_test_df_orig.drop(columns=[f for f in useless_features if f in X_test_df_orig.columns], errors='ignore')\n\nprint(f\"\\n✅ Removed {len(X_df_orig.columns) - len(X_df_cleaned.columns)} useless features\")\nprint(f\"   Remaining features: {len(X_df_cleaned.columns)}\")\n\n# ============================================================\n# STEP 2: CREATE FEATURE INTERACTIONS FOR TOP FEATURES\n# ============================================================\nprint(\"\\n\" + \"-\"*70)\nprint(\"STEP 2: CREATE FEATURE INTERACTIONS FOR TOP FEATURES\")\nprint(\"-\"*70)\n\n# Get top 10 features by SHAP importance\ntop_shap_features = shap_importance_df.head(10)['feature'].tolist()\nprint(f\"\\nTop 10 SHAP features:\")\nfor i, feat in enumerate(top_shap_features, 1):\n    shap_val = shap_importance_df[shap_importance_df['feature'] == feat]['shap_importance'].iloc[0]\n    print(f\"  {i}. {feat}: {shap_val:.4f}\")\n\n# Create interactions between top features (multiplicative)\nprint(f\"\\nCreating interactions for top i-band features...\")\ninteraction_count = 0\n\n# Focus on i-band features (they're most important)\ni_band_features = [f for f in top_shap_features if f.startswith('i_')]\nother_top_features = [f for f in top_shap_features if not f.startswith('i_')]\n\n# Create interactions: top i-feature × top other feature\nfor i_feat in i_band_features[:3]:  # Top 3 i-features\n    if i_feat not in X_df_cleaned.columns:\n        continue\n    \n    for other_feat in other_top_features[:3]:  # Top 3 other features\n        if other_feat not in X_df_cleaned.columns:\n            continue\n        \n        # Skip self-interaction\n        if i_feat == other_feat:\n            continue\n        \n        # Create multiplicative interaction\n        interaction_name = f\"{i_feat}_x_{other_feat}\"\n        \n        # Normalize before multiplication to avoid extreme values\n        i_norm = (X_df_cleaned[i_feat] - X_df_cleaned[i_feat].mean()) / (X_df_cleaned[i_feat].std() + 1e-10)\n        other_norm = (X_df_cleaned[other_feat] - X_df_cleaned[other_feat].mean()) / (X_df_cleaned[other_feat].std() + 1e-10)\n        \n        X_df_cleaned[interaction_name] = i_norm * other_norm\n        X_test_df_cleaned[interaction_name] = (\n            (X_test_df_cleaned[i_feat] - X_df_cleaned[i_feat].mean()) / (X_df_cleaned[i_feat].std() + 1e-10) *\n            (X_test_df_cleaned[other_feat] - X_df_cleaned[other_feat].mean()) / (X_df_cleaned[other_feat].std() + 1e-10)\n        )\n        interaction_count += 1\n\nprint(f\"✅ Created {interaction_count} feature interactions\")\n\n# ============================================================\n# STEP 3: CREATE CLASS-SPECIFIC FEATURES (for TDE)\n# ============================================================\nprint(\"\\n\" + \"-\"*70)\nprint(\"STEP 3: CREATE CLASS-SPECIFIC FEATURES\")\nprint(\"-\"*70)\n\n# Features important for TDE predictions (from SHAP analysis)\ntde_important_features = [\n    'i_gp2d_decline_rate',\n    'flux_range_all',\n    'i_snr_max',\n    'i_gp2d_time_fwd_0.5',\n]\n\nprint(f\"\\nCreating weighted features for TDE-important features...\")\nclass_specific_count = 0\n\nfor feat in tde_important_features:\n    if feat not in X_df_cleaned.columns:\n        continue\n    \n    # Create squared feature (non-linear transformation)\n    X_df_cleaned[f\"{feat}_squared\"] = X_df_cleaned[feat] ** 2\n    X_test_df_cleaned[f\"{feat}_squared\"] = X_test_df_cleaned[feat] ** 2\n    \n    # Create log feature if positive\n    if (X_df_cleaned[feat] > 0).all():\n        X_df_cleaned[f\"{feat}_log\"] = np.log1p(X_df_cleaned[feat])\n        X_test_df_cleaned[f\"{feat}_log\"] = np.log1p(X_test_df_cleaned[feat])\n        class_specific_count += 1\n    \n    class_specific_count += 1\n\nprint(f\"✅ Created {class_specific_count} class-specific features\")\n\n# ============================================================\n# STEP 4: CREATE ANTI-FP FEATURES (to reduce False Positives)\n# ============================================================\nprint(\"\\n\" + \"-\"*70)\nprint(\"STEP 4: CREATE ANTI-FP FEATURES\")\nprint(\"-\"*70)\n\n# Features that cause False Positives (from SHAP analysis)\nfp_features = [\n    'i_gp2d_time_fwd_0.5',\n    'i_gp2d_decline_rate',\n    'flux_range_all',\n    'u_gp2d_abs_diff',\n]\n\nprint(f\"\\nCreating anti-FP features (to reduce false positives)...\")\nanti_fp_count = 0\n\nfor feat in fp_features:\n    if feat not in X_df_cleaned.columns:\n        continue\n    \n    # Create threshold-based feature: is feature value > median?\n    median_val = X_df_cleaned[feat].median()\n    X_df_cleaned[f\"{feat}_high\"] = (X_df_cleaned[feat] > median_val).astype(float)\n    X_test_df_cleaned[f\"{feat}_high\"] = (X_test_df_cleaned[feat] > median_val).astype(float)\n    \n    anti_fp_count += 1\n\nprint(f\"✅ Created {anti_fp_count} anti-FP features\")\n\n# ============================================================\n# STEP 5: CREATE ANTI-FN FEATURES (to reduce False Negatives)\n# ============================================================\nprint(\"\\n\" + \"-\"*70)\nprint(\"STEP 5: CREATE ANTI-FN FEATURES\")\nprint(\"-\"*70)\n\n# Features important for FN cases (from SHAP analysis)\nfn_features = [\n    'u_gp2d_abs_diff',\n    'i_snr_mean',\n    'Z',\n]\n\nprint(f\"\\nCreating anti-FN features (to reduce false negatives)...\")\nanti_fn_count = 0\n\nfor feat in fn_features:\n    if feat not in X_df_cleaned.columns:\n        continue\n    \n    # Create threshold-based feature\n    median_val = X_df_cleaned[feat].median()\n    X_df_cleaned[f\"{feat}_low\"] = (X_df_cleaned[feat] < median_val).astype(float)\n    X_test_df_cleaned[f\"{feat}_low\"] = (X_test_df_cleaned[feat] < median_val).astype(float)\n    \n    anti_fn_count += 1\n\nprint(f\"✅ Created {anti_fn_count} anti-FN features\")\n\n# ============================================================\n# STEP 6: FINAL CLEANUP\n# ============================================================\nprint(\"\\n\" + \"-\"*70)\nprint(\"STEP 6: FINAL CLEANUP\")\nprint(\"-\"*70)\n\n# Ensure same columns\ncommon_cols = sorted(set(X_df_cleaned.columns) & set(X_test_df_cleaned.columns))\nX_df_cleaned = X_df_cleaned[common_cols]\nX_test_df_cleaned = X_test_df_cleaned[common_cols]\n\nprint(f\"\\nFinal feature count: {len(common_cols)}\")\nprint(f\"  - Original: {len(X_df_orig.columns)}\")\nprint(f\"  - Removed useless: {len(useless_features)}\")\nprint(f\"  - Added (interactions + transforms): {len(common_cols) - (len(X_df_orig.columns) - len([f for f in useless_features if f in X_df_orig.columns]))}\")\n\n# Replace NaN/Inf\nX_df_cleaned = X_df_cleaned.replace([np.inf, -np.inf], np.nan).fillna(0)\nX_test_df_cleaned = X_test_df_cleaned.replace([np.inf, -np.inf], np.nan).fillna(0)\n\n# Update feature names and data\nfeature_names_optimized = X_df_cleaned.columns.tolist()\nX_raw_optimized = X_df_cleaned.values\nX_test_raw_optimized = X_test_df_cleaned.values\n\nprint(f\"\\n✅ Feature optimization completed!\")\nprint(f\"   Optimized shape: Train {X_raw_optimized.shape}, Test {X_test_raw_optimized.shape}\")\n\n# Save optimized feature names\njoblib.dump(feature_names_optimized, f\"{GBM_MODEL_DIR}/optimized_feature_names.pkl\")\nprint(f\"✅ Saved optimized feature names\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-21T16:50:52.506013Z","iopub.execute_input":"2025-12-21T16:50:52.506364Z","iopub.status.idle":"2025-12-21T16:50:52.679605Z","shell.execute_reply.started":"2025-12-21T16:50:52.506311Z","shell.execute_reply":"2025-12-21T16:50:52.678538Z"}},"outputs":[{"name":"stdout","text":"\n======================================================================\nSHAP-BASED FEATURE OPTIMIZATION\n======================================================================\n\nTotal features (after SelectKBest): 246\n\n----------------------------------------------------------------------\nSTEP 1: REMOVE USELESS FEATURES\n----------------------------------------------------------------------\nFeatures to remove (SHAP < 0.001): 24\n  Examples: ['y_flux_min', 'gp2d_peak_dt_r_z', 'i_gp2d_time_bwd_0.5', 'i_gp2d_time_bwd_0.8', 'snr_median', 'r_gp2d_peaks_pos_count', 'i_gp2d_negative_width', 'i_gp2d_rise_decline_ratio', 'y_flux_mean', 'u_gp2d_fwhm']\n\n✅ Removed 24 useless features\n   Remaining features: 284\n\n----------------------------------------------------------------------\nSTEP 2: CREATE FEATURE INTERACTIONS FOR TOP FEATURES\n----------------------------------------------------------------------\n\nTop 10 SHAP features:\n  1. i_gp2d_peaks_pos_frac_2: 0.4886\n  2. i_gp2d_time_fwd_0.5: 0.4202\n  3. i_snr_max: 0.3551\n  4. i_gp2d_decline_rate: 0.3127\n  5. flux_std_all: 0.2035\n  6. flux_range_all: 0.1878\n  7. flux_max_all: 0.1817\n  8. r_gp2d_time_bwd_0.8: 0.1675\n  9. count_max_fall_30: 0.1592\n  10. r_flux_std: 0.1575\n\nCreating interactions for top i-band features...\n✅ Created 9 feature interactions\n\n----------------------------------------------------------------------\nSTEP 3: CREATE CLASS-SPECIFIC FEATURES\n----------------------------------------------------------------------\n\nCreating weighted features for TDE-important features...\n✅ Created 4 class-specific features\n\n----------------------------------------------------------------------\nSTEP 4: CREATE ANTI-FP FEATURES\n----------------------------------------------------------------------\n\nCreating anti-FP features (to reduce false positives)...\n✅ Created 4 anti-FP features\n\n----------------------------------------------------------------------\nSTEP 5: CREATE ANTI-FN FEATURES\n----------------------------------------------------------------------\n\nCreating anti-FN features (to reduce false negatives)...\n✅ Created 3 anti-FN features\n\n----------------------------------------------------------------------\nSTEP 6: FINAL CLEANUP\n----------------------------------------------------------------------\n\nFinal feature count: 304\n  - Original: 308\n  - Removed useless: 24\n  - Added (interactions + transforms): 20\n\n✅ Feature optimization completed!\n   Optimized shape: Train (3043, 304), Test (7135, 304)\n✅ Saved optimized feature names\n","output_type":"stream"}],"execution_count":33},{"cell_type":"code","source":"# ============================================================\n# CELL 20: TRAIN LIGHTGBM WITH OPTIMIZED FEATURES\n# ============================================================\n\nprint(\"\\n\" + \"=\"*60)\nprint(\"TRAINING LIGHTGBM (SHAP-OPTIMIZED FEATURES)\")\nprint(\"=\"*60)\n\n# Use optimized features\nX_raw = X_raw_optimized\nX_test_raw = X_test_raw_optimized\nfeature_names = feature_names_optimized\nn_features_original = X_raw.shape[1]\n\nprint(f\"\\nUsing optimized features: {n_features_original} features\")\n\n# Apply Robust Scaling\nprint(\"\\n--- Applying Robust Scaling ---\")\nscaler_robust_opt = RobustScaler()\nX_scaled = scaler_robust_opt.fit_transform(X_raw)\nX_test_scaled = scaler_robust_opt.transform(X_test_raw)\n\nprint(f\"Scaled features - Train: {X_scaled.shape}, Test: {X_test_scaled.shape}\")\n\n# Save scaler\njoblib.dump(scaler_robust_opt, f\"{GBM_MODEL_DIR}/robust_scaler_optimized.pkl\")\n\n# Use scaled features\nX = X_scaled\nX_test = X_test_scaled\n\n# ============================================================\n# TRAIN LIGHTGBM WITH OPTIMIZED HYPERPARAMETERS\n# ============================================================\n\n# Tuned parameters based on SHAP insights\nlgb_params_optimized = {\n    \"objective\": \"binary\",\n    \"metric\": \"binary_logloss\",\n    \"boosting_type\": \"gbdt\",\n    \"num_leaves\": 31,\n    \"max_depth\": 7,\n    \"learning_rate\": 0.03,  # Lower LR for better convergence\n    \"feature_fraction\": 0.7,  # Lower - focus on top features\n    \"bagging_fraction\": 0.8,\n    \"bagging_freq\": 5,\n    \"min_child_samples\": 30,  # Higher - more regularization\n    \"scale_pos_weight\": scale_pos_weight,\n    \"lambda_l1\": 0.1,  # L1 regularization\n    \"lambda_l2\": 0.1,  # L2 regularization\n    \"verbose\": -1,\n    \"seed\": SEED,\n    \"n_jobs\": -1,\n    \"feature_fraction_seed\": SEED,\n    \"bagging_seed\": SEED,\n    \"data_random_seed\": SEED,\n}\n\nprint(f\"\\nScale pos weight: {scale_pos_weight:.2f}\")\n\nlgb_oof_opt = np.zeros(len(y))\nlgb_test_opt = np.zeros(len(X_test))\nlgb_models_opt = []\nlgb_fold_scores_opt = []\nlgb_fold_thresholds_opt = []\nlgb_test_folds_opt = []\n\nprint(\"\\n--- Training LightGBM with 5-fold CV (Optimized) ---\")\nfor fold, (train_idx, val_idx) in enumerate(skf.split(X, y), 1):\n    print(f\"\\n--- Fold {fold}/{N_FOLDS} ---\")\n\n    X_tr, X_val = X[train_idx], X[val_idx]\n    y_tr, y_val = y[train_idx], y[val_idx]\n\n    train_data = lgb.Dataset(X_tr, label=y_tr)\n    val_data = lgb.Dataset(X_val, label=y_val, reference=train_data)\n\n    model = lgb.train(\n        lgb_params_optimized,\n        train_data,\n        num_boost_round=3000,  # More rounds with lower LR\n        valid_sets=[val_data],\n        callbacks=[lgb.early_stopping(150, verbose=False)],\n    )\n\n    lgb_models_opt.append(model)\n    fold_oof = model.predict(X_val)\n    lgb_oof_opt[val_idx] = fold_oof\n\n    fold_test = model.predict(X_test)\n    lgb_test_opt += fold_test / N_FOLDS\n    lgb_test_folds_opt.append(fold_test)\n\n    best_th, best_f1 = 0.5, 0\n    for th in np.arange(0.1, 0.9, 0.01):\n        f1 = f1_score(y_val, (fold_oof >= th).astype(int))\n        if f1 > best_f1:\n            best_f1, best_th = f1, th\n\n    lgb_fold_scores_opt.append(best_f1)\n    lgb_fold_thresholds_opt.append(best_th)\n    print(f\"Fold {fold}: F1={best_f1:.4f} at threshold={best_th:.2f}\")\n\n# Find best global threshold\ndef neg_f1(th):\n    return -f1_score(y, (lgb_oof_opt >= th).astype(int))\n\nresult = minimize_scalar(neg_f1, bounds=(0.1, 0.9), method=\"bounded\")\nlgb_best_th_opt = result.x\nlgb_best_f1_opt = -result.fun\nlgb_auc_opt = roc_auc_score(y, lgb_oof_opt)\n\nprint(f\"\\n\" + \"=\"*60)\nprint(\"OPTIMIZED RESULTS\")\nprint(\"=\"*60)\nprint(f\"\\nLightGBM (Optimized) CV F1: {np.mean(lgb_fold_scores_opt):.4f} ± {np.std(lgb_fold_scores_opt):.4f}\")\nprint(f\"LightGBM (Optimized) Global F1: {lgb_best_f1_opt:.4f} at threshold={lgb_best_th_opt:.2f}\")\nprint(f\"LightGBM (Optimized) ROC-AUC: {lgb_auc_opt:.4f}\")\n\n# Compare with previous\nif 'lgb_best_f1' in locals():\n    improvement = lgb_best_f1_opt - lgb_best_f1\n    improvement_pct = improvement / lgb_best_f1 * 100 if lgb_best_f1 > 0 else 0\n    print(f\"\\n\" + \"-\"*60)\n    print(\"COMPARISON WITH PREVIOUS MODEL\")\n    print(\"-\"*60)\n    print(f\"Previous F1: {lgb_best_f1:.4f}\")\n    print(f\"Optimized F1: {lgb_best_f1_opt:.4f}\")\n    print(f\"Improvement: {improvement:+.4f} ({improvement_pct:+.2f}%)\")\n\n# Save models\nfor i, model in enumerate(lgb_models_opt):\n    model.save_model(f\"{GBM_MODEL_DIR}/lgb_model_optimized_fold{i+1}.txt\")\n\n# Save predictions and config\nnp.save(f\"{GBM_MODEL_DIR}/lgb_oof_optimized.npy\", lgb_oof_opt)\nnp.save(f\"{GBM_MODEL_DIR}/lgb_test_optimized.npy\", lgb_test_opt)\n\nlgb_config_opt = {\n    \"SEED\": SEED,\n    \"n_features\": X.shape[1],\n    \"n_features_original\": len(feature_names),\n    \"scale_pos_weight\": scale_pos_weight,\n    \"lgb_params\": lgb_params_optimized,\n    \"fold_scores\": lgb_fold_scores_opt,\n    \"fold_thresholds\": lgb_fold_thresholds_opt,\n    \"best_threshold\": lgb_best_th_opt,\n    \"best_f1\": lgb_best_f1_opt,\n    \"roc_auc\": lgb_auc_opt,\n    \"cv_f1_mean\": float(np.mean(lgb_fold_scores_opt)),\n    \"cv_f1_std\": float(np.std(lgb_fold_scores_opt)),\n}\njoblib.dump(lgb_config_opt, f\"{GBM_MODEL_DIR}/lgb_config_optimized.pkl\")\n\n# Submission\nbest_lgb_fold_opt = int(np.argmax(lgb_fold_scores_opt))\nbest_lgb_th_opt = float(lgb_fold_thresholds_opt[best_lgb_fold_opt])\nbest_lgb_test_opt = np.asarray(lgb_test_folds_opt[best_lgb_fold_opt], dtype=float)\n\nlgb_single_preds_opt = (best_lgb_test_opt >= best_lgb_th_opt).astype(int)\nsub_lgb_single_opt = pd.DataFrame({\n    \"object_id\": test_log[\"object_id\"],\n    \"target\": lgb_single_preds_opt\n})\nsub_lgb_single_opt.to_csv(\"submission_LGB_SHAP_optimized.csv\", index=False)\nprint(f\"\\n✅ Saved optimized submission -> submission_LGB_SHAP_optimized.csv\")\n\nprint(\"\\n\" + \"=\"*60)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-21T16:51:08.810442Z","iopub.execute_input":"2025-12-21T16:51:08.811215Z","iopub.status.idle":"2025-12-21T16:51:28.173051Z","shell.execute_reply.started":"2025-12-21T16:51:08.811164Z","shell.execute_reply":"2025-12-21T16:51:28.171984Z"}},"outputs":[{"name":"stdout","text":"\n============================================================\nTRAINING LIGHTGBM (SHAP-OPTIMIZED FEATURES)\n============================================================\n\nUsing optimized features: 304 features\n\n--- Applying Robust Scaling ---\nScaled features - Train: (3043, 304), Test: (7135, 304)\n\nScale pos weight: 19.56\n\n--- Training LightGBM with 5-fold CV (Optimized) ---\n\n--- Fold 1/5 ---\nFold 1: F1=0.5455 at threshold=0.30\n\n--- Fold 2/5 ---\nFold 2: F1=0.5556 at threshold=0.50\n\n--- Fold 3/5 ---\nFold 3: F1=0.5000 at threshold=0.17\n\n--- Fold 4/5 ---\nFold 4: F1=0.6667 at threshold=0.55\n\n--- Fold 5/5 ---\nFold 5: F1=0.5479 at threshold=0.20\n\n============================================================\nOPTIMIZED RESULTS\n============================================================\n\nLightGBM (Optimized) CV F1: 0.5631 ± 0.0553\nLightGBM (Optimized) Global F1: 0.5196 at threshold=0.42\nLightGBM (Optimized) ROC-AUC: 0.9473\n\n------------------------------------------------------------\nCOMPARISON WITH PREVIOUS MODEL\n------------------------------------------------------------\nPrevious F1: 0.5631\nOptimized F1: 0.5196\nImprovement: -0.0435 (-7.73%)\n\n✅ Saved optimized submission -> submission_LGB_SHAP_optimized.csv\n\n============================================================\n","output_type":"stream"}],"execution_count":34},{"cell_type":"code","source":"# ============================================================\n# CELL 21: HYPERPARAMETER TUNING BASED ON SHAP INSIGHTS\n# ============================================================\n\nprint(\"\\n\" + \"=\"*70)\nprint(\"HYPERPARAMETER TUNING BASED ON SHAP INSIGHTS\")\nprint(\"=\"*70)\n\n# Key insight từ SHAP: \n# - i-band features rất quan trọng cho TDE\n# - Model có class-specific behavior\n# - Cần focus vào class weighting và feature importance\n\n# Restore original features (before SHAP optimization)\nprint(\"\\nRestoring original features (before SHAP optimization)...\")\nX_df_orig = train_feat.copy()\nX_test_df_orig = test_feat.copy()\n\nif \"object_id\" in X_df_orig.columns:\n    X_df_orig = X_df_orig.drop(columns=[\"object_id\"])\nif \"object_id\" in X_test_df_orig.columns:\n    X_test_df_orig = X_test_df_orig.drop(columns=[\"object_id\"])\n\ncommon_cols_orig = sorted(set(X_df_orig.columns) & set(X_test_df_orig.columns))\nX_df_orig = X_df_orig[common_cols_orig]\nX_test_df_orig = X_test_df_orig[common_cols_orig]\n\nX_df_orig = X_df_orig.replace([np.inf, -np.inf], np.nan).fillna(0)\nX_test_df_orig = X_test_df_orig.replace([np.inf, -np.inf], np.nan).fillna(0)\n\n# Apply SelectKBest (giống như trước)\nif USE_SELECTKBEST and len(common_cols_orig) > K_BEST_MIN:\n    from sklearn.feature_selection import SelectKBest, f_classif\n    \n    feature_selector_restored = SelectKBest(score_func=f_classif, k=int(len(common_cols_orig) * K_BEST_RATIO))\n    X_restored = feature_selector_restored.fit_transform(X_df_orig.values, y)\n    X_test_restored = feature_selector_restored.transform(X_test_df_orig.values)\n    \n    selected_mask = feature_selector_restored.get_support()\n    selected_feature_names_restored = [common_cols_orig[i] for i in range(len(common_cols_orig)) if selected_mask[i]]\n    \n    print(f\"✅ Restored {len(selected_feature_names_restored)} features (SelectKBest)\")\nelse:\n    X_restored = X_df_orig.values\n    X_test_restored = X_test_df_orig.values\n    selected_feature_names_restored = common_cols_orig\n    print(f\"✅ Restored {len(selected_feature_names_restored)} features (no selection)\")\n\n# Apply Robust Scaling\nscaler_restored = RobustScaler()\nX_restored = scaler_restored.fit_transform(X_restored)\nX_test_restored = scaler_restored.transform(X_test_restored)\n\n# Use restored data\nX = X_restored\nX_test = X_test_restored\nfeature_names = selected_feature_names_restored\n\nprint(f\"\\nFinal shape: Train {X.shape}, Test {X_test.shape}\")\n\n# ============================================================\n# TUNED HYPERPARAMETERS BASED ON SHAP INSIGHTS\n# ============================================================\n\n# Key changes based on SHAP:\n# 1. Higher min_child_samples (reduce overfitting on minority class)\n# 2. Tuned feature_fraction (focus on discriminative features)\n# 3. Adjusted regularization\n# 4. Better class weighting\n\nlgb_params_tuned = {\n    \"objective\": \"binary\",\n    \"metric\": \"binary_logloss\",\n    \"boosting_type\": \"gbdt\",\n    \"num_leaves\": 31,\n    \"max_depth\": 6,  # Slightly lower to reduce complexity\n    \"learning_rate\": 0.05,  # Keep original\n    \"feature_fraction\": 0.85,  # Higher - SHAP shows most features are useful\n    \"bagging_fraction\": 0.85,\n    \"bagging_freq\": 5,\n    \"min_child_samples\": 25,  # Higher for regularization\n    \"scale_pos_weight\": scale_pos_weight,\n    \"lambda_l1\": 0.05,  # Light L1 regularization\n    \"lambda_l2\": 0.05,  # Light L2 regularization\n    \"min_gain_to_split\": 0.01,  # Require minimum gain\n    \"verbose\": -1,\n    \"seed\": SEED,\n    \"n_jobs\": -1,\n    \"feature_fraction_seed\": SEED,\n    \"bagging_seed\": SEED,\n    \"data_random_seed\": SEED,\n}\n\nprint(f\"\\n--- Hyperparameters tuned based on SHAP insights ---\")\nprint(f\"  - max_depth: 7 → 6 (reduce complexity)\")\nprint(f\"  - feature_fraction: 0.8 → 0.85 (keep more features)\")\nprint(f\"  - min_child_samples: 20 → 25 (more regularization)\")\nprint(f\"  - Added L1/L2 regularization: 0.05 each\")\nprint(f\"  - min_gain_to_split: 0.01 (require meaningful splits)\")\n\nlgb_oof_tuned = np.zeros(len(y))\nlgb_test_tuned = np.zeros(len(X_test))\nlgb_models_tuned = []\nlgb_fold_scores_tuned = []\nlgb_fold_thresholds_tuned = []\nlgb_test_folds_tuned = []\n\nprint(\"\\n--- Training LightGBM with Tuned Hyperparameters ---\")\nfor fold, (train_idx, val_idx) in enumerate(skf.split(X, y), 1):\n    print(f\"\\n--- Fold {fold}/{N_FOLDS} ---\")\n\n    X_tr, X_val = X[train_idx], X[val_idx]\n    y_tr, y_val = y[train_idx], y[val_idx]\n\n    train_data = lgb.Dataset(X_tr, label=y_tr)\n    val_data = lgb.Dataset(X_val, label=y_val, reference=train_data)\n\n    model = lgb.train(\n        lgb_params_tuned,\n        train_data,\n        num_boost_round=2500,  # Slightly more rounds\n        valid_sets=[val_data],\n        callbacks=[lgb.early_stopping(150, verbose=False)],\n    )\n\n    lgb_models_tuned.append(model)\n    fold_oof = model.predict(X_val)\n    lgb_oof_tuned[val_idx] = fold_oof\n\n    fold_test = model.predict(X_test)\n    lgb_test_tuned += fold_test / N_FOLDS\n    lgb_test_folds_tuned.append(fold_test)\n\n    best_th, best_f1 = 0.5, 0\n    for th in np.arange(0.1, 0.9, 0.01):\n        f1 = f1_score(y_val, (fold_oof >= th).astype(int))\n        if f1 > best_f1:\n            best_f1, best_th = f1, th\n\n    lgb_fold_scores_tuned.append(best_f1)\n    lgb_fold_thresholds_tuned.append(best_th)\n    print(f\"Fold {fold}: F1={best_f1:.4f} at threshold={best_th:.2f}\")\n\n# Find best global threshold\ndef neg_f1(th):\n    return -f1_score(y, (lgb_oof_tuned >= th).astype(int))\n\nresult = minimize_scalar(neg_f1, bounds=(0.1, 0.9), method=\"bounded\")\nlgb_best_th_tuned = result.x\nlgb_best_f1_tuned = -result.fun\nlgb_auc_tuned = roc_auc_score(y, lgb_oof_tuned)\n\nprint(f\"\\n\" + \"=\"*60)\nprint(\"TUNED RESULTS\")\nprint(\"=\"*60)\nprint(f\"\\nLightGBM (Tuned) CV F1: {np.mean(lgb_fold_scores_tuned):.4f} ± {np.std(lgb_fold_scores_tuned):.4f}\")\nprint(f\"LightGBM (Tuned) Global F1: {lgb_best_f1_tuned:.4f} at threshold={lgb_best_th_tuned:.2f}\")\nprint(f\"LightGBM (Tuned) ROC-AUC: {lgb_auc_tuned:.4f}\")\n\n# Compare with original\nif 'lgb_best_f1' in locals():\n    improvement = lgb_best_f1_tuned - lgb_best_f1\n    improvement_pct = improvement / lgb_best_f1 * 100 if lgb_best_f1 > 0 else 0\n    print(f\"\\n\" + \"-\"*60)\n    print(\"COMPARISON WITH ORIGINAL MODEL\")\n    print(\"-\"*60)\n    print(f\"Original F1: {lgb_best_f1:.4f}\")\n    print(f\"Tuned F1: {lgb_best_f1_tuned:.4f}\")\n    print(f\"Improvement: {improvement:+.4f} ({improvement_pct:+.2f}%)\")\n\n# Save\nfor i, model in enumerate(lgb_models_tuned):\n    model.save_model(f\"{GBM_MODEL_DIR}/lgb_model_tuned_fold{i+1}.txt\")\n\nnp.save(f\"{GBM_MODEL_DIR}/lgb_oof_tuned.npy\", lgb_oof_tuned)\nnp.save(f\"{GBM_MODEL_DIR}/lgb_test_tuned.npy\", lgb_test_tuned)\n\n# Submission\nbest_lgb_fold_tuned = int(np.argmax(lgb_fold_scores_tuned))\nbest_lgb_th_tuned = float(lgb_fold_thresholds_tuned[best_lgb_fold_tuned])\nbest_lgb_test_tuned = np.asarray(lgb_test_folds_tuned[best_lgb_fold_tuned], dtype=float)\n\nlgb_single_preds_tuned = (best_lgb_test_tuned >= best_lgb_th_tuned).astype(int)\nsub_lgb_tuned = pd.DataFrame({\n    \"object_id\": test_log[\"object_id\"],\n    \"target\": lgb_single_preds_tuned\n})\nsub_lgb_tuned.to_csv(\"submission_LGB_tuned_hyperparams.csv\", index=False)\nprint(f\"\\n✅ Saved tuned submission -> submission_LGB_tuned_hyperparams.csv\")\n\nprint(\"\\n\" + \"=\"*60)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-21T16:54:01.704656Z","iopub.execute_input":"2025-12-21T16:54:01.705040Z","iopub.status.idle":"2025-12-21T16:54:16.126865Z","shell.execute_reply.started":"2025-12-21T16:54:01.705009Z","shell.execute_reply":"2025-12-21T16:54:16.125932Z"}},"outputs":[{"name":"stdout","text":"\n======================================================================\nHYPERPARAMETER TUNING BASED ON SHAP INSIGHTS\n======================================================================\n\nRestoring original features (before SHAP optimization)...\n✅ Restored 246 features (SelectKBest)\n\nFinal shape: Train (3043, 246), Test (7135, 246)\n\n--- Hyperparameters tuned based on SHAP insights ---\n  - max_depth: 7 → 6 (reduce complexity)\n  - feature_fraction: 0.8 → 0.85 (keep more features)\n  - min_child_samples: 20 → 25 (more regularization)\n  - Added L1/L2 regularization: 0.05 each\n  - min_gain_to_split: 0.01 (require meaningful splits)\n\n--- Training LightGBM with Tuned Hyperparameters ---\n\n--- Fold 1/5 ---\nFold 1: F1=0.5926 at threshold=0.10\n\n--- Fold 2/5 ---\nFold 2: F1=0.5763 at threshold=0.35\n\n--- Fold 3/5 ---\nFold 3: F1=0.5532 at threshold=0.48\n\n--- Fold 4/5 ---\nFold 4: F1=0.6792 at threshold=0.48\n\n--- Fold 5/5 ---\nFold 5: F1=0.6061 at threshold=0.34\n\n============================================================\nTUNED RESULTS\n============================================================\n\nLightGBM (Tuned) CV F1: 0.6015 ± 0.0427\nLightGBM (Tuned) Global F1: 0.5545 at threshold=0.31\nLightGBM (Tuned) ROC-AUC: 0.9495\n\n------------------------------------------------------------\nCOMPARISON WITH ORIGINAL MODEL\n------------------------------------------------------------\nOriginal F1: 0.5631\nTuned F1: 0.5545\nImprovement: -0.0086 (-1.53%)\n\n✅ Saved tuned submission -> submission_LGB_tuned_hyperparams.csv\n\n============================================================\n","output_type":"stream"}],"execution_count":35},{"cell_type":"code","source":"# ============================================================\n# CELL 22: IMPROVED THRESHOLD OPTIMIZATION\n# ============================================================\n\nprint(\"\\n\" + \"=\"*70)\nprint(\"IMPROVED THRESHOLD OPTIMIZATION\")\nprint(\"=\"*70)\n\n# Use tuned model predictions\nlgb_oof = lgb_oof_tuned\nlgb_test = lgb_test_tuned\n\nprint(f\"\\nCV F1 Scores: {lgb_fold_scores_tuned}\")\nprint(f\"CV F1 Mean: {np.mean(lgb_fold_scores_tuned):.4f} ± {np.std(lgb_fold_scores_tuned):.4f}\")\n\n# Method 1: Use median of fold thresholds (more stable)\nmedian_threshold = np.median(lgb_fold_thresholds_tuned)\nf1_median_th = f1_score(y, (lgb_oof >= median_threshold).astype(int))\n\nprint(f\"\\n1. Median Threshold Method:\")\nprint(f\"   Threshold: {median_threshold:.4f}\")\nprint(f\"   F1 Score: {f1_median_th:.4f}\")\n\n# Method 2: Weighted average of fold thresholds (weighted by fold F1)\nweights = np.array(lgb_fold_scores_tuned)\nweights = weights / weights.sum()\nweighted_threshold = np.average(lgb_fold_thresholds_tuned, weights=weights)\nf1_weighted_th = f1_score(y, (lgb_oof >= weighted_threshold).astype(int))\n\nprint(f\"\\n2. Weighted Average Threshold Method:\")\nprint(f\"   Threshold: {weighted_threshold:.4f}\")\nprint(f\"   F1 Score: {f1_weighted_th:.4f}\")\n\n# Method 3: Optimize on full OOF predictions (original method)\ndef neg_f1(th):\n    return -f1_score(y, (lgb_oof >= th).astype(int))\n\nresult = minimize_scalar(neg_f1, bounds=(0.1, 0.9), method=\"bounded\")\noptimal_threshold = result.x\nf1_optimal = -result.fun\n\nprint(f\"\\n3. Optimal Threshold Method (full OOF):\")\nprint(f\"   Threshold: {optimal_threshold:.4f}\")\nprint(f\"   F1 Score: {f1_optimal:.4f}\")\n\n# Method 4: Use best fold's threshold\nbest_fold_idx = int(np.argmax(lgb_fold_scores_tuned))\nbest_fold_threshold = lgb_fold_thresholds_tuned[best_fold_idx]\nf1_best_fold_th = f1_score(y, (lgb_oof >= best_fold_threshold).astype(int))\n\nprint(f\"\\n4. Best Fold Threshold Method:\")\nprint(f\"   Threshold: {best_fold_threshold:.4f} (from fold {best_fold_idx+1})\")\nprint(f\"   F1 Score: {f1_best_fold_th:.4f}\")\n\n# Compare all methods\nthreshold_methods = {\n    'Median': (median_threshold, f1_median_th),\n    'Weighted': (weighted_threshold, f1_weighted_th),\n    'Optimal': (optimal_threshold, f1_optimal),\n    'Best Fold': (best_fold_threshold, f1_best_fold_th),\n}\n\nbest_method = max(threshold_methods.items(), key=lambda x: x[1][1])\nprint(f\"\\n\" + \"=\"*70)\nprint(f\"BEST METHOD: {best_method[0]}\")\nprint(f\"  Threshold: {best_method[1][0]:.4f}\")\nprint(f\"  F1 Score: {best_method[1][1]:.4f}\")\nprint(\"=\"*70)\n\n# Use best threshold for submission\nfinal_threshold = best_method[1][0]\nfinal_f1 = best_method[1][1]\n\n# Predictions with best threshold\nlgb_test_preds = (lgb_test >= final_threshold).astype(int)\n\nsub_lgb_final = pd.DataFrame({\n    \"object_id\": test_log[\"object_id\"],\n    \"target\": lgb_test_preds\n})\nsub_lgb_final.to_csv(\"submission_LGB_improved_threshold.csv\", index=False)\nprint(f\"\\n✅ Saved submission with improved threshold -> submission_LGB_improved_threshold.csv\")\nprint(f\"   Final F1: {final_f1:.4f}, Threshold: {final_threshold:.4f}\")\n\n# Save config\nlgb_config_final = {\n    \"SEED\": SEED,\n    \"n_features\": X.shape[1],\n    \"scale_pos_weight\": scale_pos_weight,\n    \"lgb_params\": lgb_params_tuned,\n    \"fold_scores\": lgb_fold_scores_tuned,\n    \"fold_thresholds\": lgb_fold_thresholds_tuned,\n    \"threshold_method\": best_method[0],\n    \"best_threshold\": float(final_threshold),\n    \"best_f1\": float(final_f1),\n    \"roc_auc\": float(lgb_auc_tuned),\n    \"cv_f1_mean\": float(np.mean(lgb_fold_scores_tuned)),\n    \"cv_f1_std\": float(np.std(lgb_fold_scores_tuned)),\n}\njoblib.dump(lgb_config_final, f\"{GBM_MODEL_DIR}/lgb_config_final.pkl\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-21T16:56:44.030027Z","iopub.execute_input":"2025-12-21T16:56:44.030392Z","iopub.status.idle":"2025-12-21T16:56:44.125456Z","shell.execute_reply.started":"2025-12-21T16:56:44.030364Z","shell.execute_reply":"2025-12-21T16:56:44.124453Z"}},"outputs":[{"name":"stdout","text":"\n======================================================================\nIMPROVED THRESHOLD OPTIMIZATION\n======================================================================\n\nCV F1 Scores: [0.5925925925925926, 0.576271186440678, 0.5531914893617021, 0.6792452830188679, 0.6060606060606061]\nCV F1 Mean: 0.6015 ± 0.0427\n\n1. Median Threshold Method:\n   Threshold: 0.3500\n   F1 Score: 0.5369\n\n2. Weighted Average Threshold Method:\n   Threshold: 0.3520\n   F1 Score: 0.5369\n\n3. Optimal Threshold Method (full OOF):\n   Threshold: 0.3065\n   F1 Score: 0.5545\n\n4. Best Fold Threshold Method:\n   Threshold: 0.4800 (from fold 4)\n   F1 Score: 0.5649\n\n======================================================================\nBEST METHOD: Best Fold\n  Threshold: 0.4800\n  F1 Score: 0.5649\n======================================================================\n\n✅ Saved submission with improved threshold -> submission_LGB_improved_threshold.csv\n   Final F1: 0.5649, Threshold: 0.4800\n","output_type":"stream"},{"execution_count":36,"output_type":"execute_result","data":{"text/plain":"['/kaggle/working/models/lgb_config_final.pkl']"},"metadata":{}}],"execution_count":36},{"cell_type":"code","source":"# ============================================================\n# CELL 24: PREDICTION DISTRIBUTION ANALYSIS & CALIBRATION\n# ============================================================\n\nprint(\"\\n\" + \"=\"*70)\nprint(\"PREDICTION DISTRIBUTION ANALYSIS\")\nprint(\"=\"*70)\n\n# Analyze OOF predictions distribution\nprint(\"\\n1. OOF Prediction Distribution:\")\nprint(f\"   Min: {lgb_oof_tuned.min():.4f}\")\nprint(f\"   Max: {lgb_oof_tuned.max():.4f}\")\nprint(f\"   Mean: {lgb_oof_tuned.mean():.4f}\")\nprint(f\"   Median: {np.median(lgb_oof_tuned):.4f}\")\nprint(f\"   Std: {lgb_oof_tuned.std():.4f}\")\n\nprint(\"\\n2. Prediction Distribution by Class:\")\nprint(f\"   TDE (y=1):\")\nprint(f\"     Mean: {lgb_oof_tuned[y == 1].mean():.4f}\")\nprint(f\"     Median: {np.median(lgb_oof_tuned[y == 1]):.4f}\")\nprint(f\"     Std: {lgb_oof_tuned[y == 1].std():.4f}\")\nprint(f\"     Min: {lgb_oof_tuned[y == 1].min():.4f}\")\nprint(f\"     Max: {lgb_oof_tuned[y == 1].max():.4f}\")\n\nprint(f\"\\n   Non-TDE (y=0):\")\nprint(f\"     Mean: {lgb_oof_tuned[y == 0].mean():.4f}\")\nprint(f\"     Median: {np.median(lgb_oof_tuned[y == 0]):.4f}\")\nprint(f\"     Std: {lgb_oof_tuned[y == 0].std():.4f}\")\nprint(f\"     Min: {lgb_oof_tuned[y == 0].min():.4f}\")\nprint(f\"     Max: {lgb_oof_tuned[y == 0].max():.4f}\")\n\nseparation = lgb_oof_tuned[y == 1].mean() - lgb_oof_tuned[y == 0].mean()\nprint(f\"\\n   Separation: {separation:.4f}\")\n\n# Prediction bins analysis\nprint(\"\\n3. Prediction Bins Analysis:\")\nbins = np.linspace(0, 1, 21)\nbin_centers = (bins[:-1] + bins[1:]) / 2\n\nfor i in range(len(bins) - 1):\n    mask = (lgb_oof_tuned >= bins[i]) & (lgb_oof_tuned < bins[i+1])\n    if i == len(bins) - 2:\n        mask = (lgb_oof_tuned >= bins[i]) & (lgb_oof_tuned <= bins[i+1])\n    \n    count = mask.sum()\n    if count > 0:\n        tde_count = y[mask].sum()\n        tde_rate = tde_count / count\n        print(f\"   [{bins[i]:.2f}, {bins[i+1]:.2f}]: {count:4d} samples, {tde_count:3d} TDE ({tde_rate*100:5.1f}%)\")\n\n# Find optimal threshold ranges\nprint(\"\\n4. Threshold Range Analysis:\")\nbest_thresholds_per_fold = lgb_fold_thresholds_tuned\nprint(f\"   Fold thresholds: {[f'{t:.3f}' for t in best_thresholds_per_fold]}\")\nprint(f\"   Mean: {np.mean(best_thresholds_per_fold):.4f}\")\nprint(f\"   Std: {np.std(best_thresholds_per_fold):.4f}\")\nprint(f\"   Range: [{np.min(best_thresholds_per_fold):.4f}, {np.max(best_thresholds_per_fold):.4f}]\")\n\n# Check if predictions are well-calibrated\nprint(\"\\n5. Calibration Check:\")\nfrom sklearn.calibration import calibration_curve\n\nfraction_of_positives, mean_predicted_value = calibration_curve(\n    y, lgb_oof_tuned, n_bins=10, strategy='uniform'\n)\n\ncalibration_error = np.mean(np.abs(fraction_of_positives - mean_predicted_value))\nprint(f\"   Mean Calibration Error: {calibration_error:.4f}\")\nif calibration_error < 0.05:\n    print(\"   ✅ Well calibrated\")\nelif calibration_error < 0.10:\n    print(\"   ⚠️  Moderately calibrated\")\nelse:\n    print(\"   ❌ Poorly calibrated\")\n\nprint(\"\\n\" + \"=\"*70)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-21T16:59:49.383608Z","iopub.execute_input":"2025-12-21T16:59:49.384459Z","iopub.status.idle":"2025-12-21T16:59:49.406701Z","shell.execute_reply.started":"2025-12-21T16:59:49.384422Z","shell.execute_reply":"2025-12-21T16:59:49.405693Z"}},"outputs":[{"name":"stdout","text":"\n======================================================================\nPREDICTION DISTRIBUTION ANALYSIS\n======================================================================\n\n1. OOF Prediction Distribution:\n   Min: 0.0000\n   Max: 0.9970\n   Mean: 0.0469\n   Median: 0.0005\n   Std: 0.1559\n\n2. Prediction Distribution by Class:\n   TDE (y=1):\n     Mean: 0.4575\n     Median: 0.4551\n     Std: 0.3434\n     Min: 0.0003\n     Max: 0.9970\n\n   Non-TDE (y=0):\n     Mean: 0.0259\n     Median: 0.0004\n     Std: 0.1023\n     Min: 0.0000\n     Max: 0.9753\n\n   Separation: 0.4316\n\n3. Prediction Bins Analysis:\n   [0.00, 0.05]: 2673 samples,  23 TDE (  0.9%)\n   [0.05, 0.10]:   89 samples,   9 TDE ( 10.1%)\n   [0.10, 0.15]:   43 samples,  12 TDE ( 27.9%)\n   [0.15, 0.20]:   28 samples,  10 TDE ( 35.7%)\n   [0.20, 0.25]:   19 samples,   4 TDE ( 21.1%)\n   [0.25, 0.30]:   15 samples,   1 TDE (  6.7%)\n   [0.30, 0.35]:   26 samples,   9 TDE ( 34.6%)\n   [0.35, 0.40]:   10 samples,   4 TDE ( 40.0%)\n   [0.40, 0.45]:   17 samples,   2 TDE ( 11.8%)\n   [0.45, 0.50]:   14 samples,   3 TDE ( 21.4%)\n   [0.50, 0.55]:   12 samples,   5 TDE ( 41.7%)\n   [0.55, 0.60]:   12 samples,   8 TDE ( 66.7%)\n   [0.60, 0.65]:    9 samples,   4 TDE ( 44.4%)\n   [0.65, 0.70]:    6 samples,   5 TDE ( 83.3%)\n   [0.70, 0.75]:   10 samples,   7 TDE ( 70.0%)\n   [0.75, 0.80]:   13 samples,  10 TDE ( 76.9%)\n   [0.80, 0.85]:   10 samples,   6 TDE ( 60.0%)\n   [0.85, 0.90]:   10 samples,   4 TDE ( 40.0%)\n   [0.90, 0.95]:   10 samples,   7 TDE ( 70.0%)\n   [0.95, 1.00]:   17 samples,  15 TDE ( 88.2%)\n\n4. Threshold Range Analysis:\n   Fold thresholds: ['0.100', '0.350', '0.480', '0.480', '0.340']\n   Mean: 0.3500\n   Std: 0.1389\n   Range: [0.1000, 0.4800]\n\n5. Calibration Check:\n   Mean Calibration Error: 0.1141\n   ❌ Poorly calibrated\n\n======================================================================\n","output_type":"stream"}],"execution_count":37},{"cell_type":"code","source":"# ============================================================\n# CELL 26: PREDICTION CALIBRATION\n# ============================================================\n\nprint(\"\\n\" + \"=\"*70)\nprint(\"PREDICTION CALIBRATION\")\nprint(\"=\"*70)\n\nfrom sklearn.calibration import CalibratedClassifierCV, CalibrationDisplay\nfrom sklearn.isotonic import IsotonicRegression\nfrom sklearn.linear_model import LogisticRegression\n\n# Current predictions\nlgb_oof_cal = lgb_oof_tuned.copy()\nlgb_test_cal = lgb_test_tuned.copy()\n\nprint(\"\\n1. Applying Isotonic Regression Calibration...\")\n\n# Fit calibration on OOF predictions\niso_reg = IsotonicRegression(out_of_bounds='clip')\niso_reg.fit(lgb_oof_cal, y)\n\n# Apply calibration\nlgb_oof_calibrated = iso_reg.transform(lgb_oof_cal)\nlgb_test_calibrated = iso_reg.transform(lgb_test_cal)\n\n# Check calibration improvement\nfrom sklearn.calibration import calibration_curve\nfraction_of_positives_orig, mean_predicted_value_orig = calibration_curve(\n    y, lgb_oof_cal, n_bins=10, strategy='uniform'\n)\nfraction_of_positives_cal, mean_predicted_value_cal = calibration_curve(\n    y, lgb_oof_calibrated, n_bins=10, strategy='uniform'\n)\n\ncal_error_orig = np.mean(np.abs(fraction_of_positives_orig - mean_predicted_value_orig))\ncal_error_cal = np.mean(np.abs(fraction_of_positives_cal - mean_predicted_value_cal))\n\nprint(f\"\\n   Original Calibration Error: {cal_error_orig:.4f}\")\nprint(f\"   Calibrated Error: {cal_error_cal:.4f}\")\nprint(f\"   Improvement: {cal_error_orig - cal_error_cal:.4f}\")\n\n# Re-optimize threshold on calibrated predictions\ndef neg_f1_cal(th):\n    return -f1_score(y, (lgb_oof_calibrated >= th).astype(int))\n\nresult_cal = minimize_scalar(neg_f1_cal, bounds=(0.1, 0.9), method=\"bounded\")\nbest_th_cal = result_cal.x\nbest_f1_cal = -result_cal.fun\n\nprint(f\"\\n2. Threshold Optimization on Calibrated Predictions:\")\nprint(f\"   Optimal threshold: {best_th_cal:.4f}\")\nprint(f\"   F1 Score: {best_f1_cal:.4f}\")\n\n# Compare with original\nif 'lgb_best_f1_tuned' in locals():\n    improvement = best_f1_cal - lgb_best_f1_tuned\n    print(f\"\\n   Original F1: {lgb_best_f1_tuned:.4f}\")\n    print(f\"   Calibrated F1: {best_f1_cal:.4f}\")\n    print(f\"   Improvement: {improvement:+.4f}\")\n\n# Save calibration model\njoblib.dump(iso_reg, f\"{GBM_MODEL_DIR}/isotonic_calibration.pkl\")\nprint(f\"\\n✅ Calibration applied and saved!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-21T17:01:57.442020Z","iopub.execute_input":"2025-12-21T17:01:57.442618Z","iopub.status.idle":"2025-12-21T17:01:57.517363Z","shell.execute_reply.started":"2025-12-21T17:01:57.442587Z","shell.execute_reply":"2025-12-21T17:01:57.516424Z"}},"outputs":[{"name":"stdout","text":"\n======================================================================\nPREDICTION CALIBRATION\n======================================================================\n\n1. Applying Isotonic Regression Calibration...\n\n   Original Calibration Error: 0.1141\n   Calibrated Error: 0.0000\n   Improvement: 0.1141\n\n2. Threshold Optimization on Calibrated Predictions:\n   Optimal threshold: 0.4056\n   F1 Score: 0.5455\n\n   Original F1: 0.5545\n   Calibrated F1: 0.5455\n   Improvement: -0.0091\n\n✅ Calibration applied and saved!\n","output_type":"stream"}],"execution_count":38},{"cell_type":"code","source":"# ============================================================\n# CELL 27: FINAL MODEL WITH CALIBRATION + REGULARIZATION\n# ============================================================\n\nprint(\"\\n\" + \"=\"*70)\nprint(\"FINAL MODEL: CALIBRATION + STRONG REGULARIZATION\")\nprint(\"=\"*70)\n\n# Strategy: \n# - Strong regularization to get more stable predictions\n# - Better calibration\n# - Focus on threshold stability\n\nlgb_params_final = {\n    \"objective\": \"binary\",\n    \"metric\": \"binary_logloss\",\n    \"boosting_type\": \"gbdt\",\n    \"num_leaves\": 24,  # Further reduced\n    \"max_depth\": 4,  # Further reduced\n    \"learning_rate\": 0.03,  # Lower\n    \"feature_fraction\": 0.75,  # Reduced\n    \"bagging_fraction\": 0.75,\n    \"bagging_freq\": 5,\n    \"min_child_samples\": 40,  # Much higher\n    \"min_data_in_leaf\": 30,  # Higher\n    \"scale_pos_weight\": scale_pos_weight,\n    \"lambda_l1\": 0.2,  # Stronger\n    \"lambda_l2\": 0.2,  # Stronger\n    \"min_gain_to_split\": 0.03,  # Higher\n    \"max_bin\": 200,  # Reduced\n    \"subsample_for_bin\": 200000,\n    \"verbose\": -1,\n    \"seed\": SEED,\n    \"n_jobs\": -1,\n    \"feature_fraction_seed\": SEED,\n    \"bagging_seed\": SEED,\n    \"data_random_seed\": SEED,\n}\n\nprint(\"\\n--- Final Hyperparameters (Strong Regularization) ---\")\nprint(f\"  - num_leaves: 28 → 24\")\nprint(f\"  - max_depth: 5 → 4\")\nprint(f\"  - learning_rate: 0.04 → 0.03\")\nprint(f\"  - feature_fraction: 0.8 → 0.75\")\nprint(f\"  - min_child_samples: 30 → 40\")\nprint(f\"  - min_data_in_leaf: 20 → 30\")\nprint(f\"  - lambda_l1/l2: 0.1 → 0.2\")\nprint(f\"  - min_gain_to_split: 0.02 → 0.03\")\nprint(f\"  - max_bin: 255 → 200\")\nprint(f\"  → Goal: Stable, well-calibrated predictions\")\n\nlgb_oof_final_raw = np.zeros(len(y))\nlgb_test_final_raw = np.zeros(len(X_test))\nlgb_models_final = []\nlgb_fold_scores_final = []\nlgb_fold_thresholds_final = []\nlgb_test_folds_final = []\n\nprint(\"\\n--- Training Final Model ---\")\nfor fold, (train_idx, val_idx) in enumerate(skf.split(X, y), 1):\n    print(f\"\\n--- Fold {fold}/{N_FOLDS} ---\")\n\n    X_tr, X_val = X[train_idx], X[val_idx]\n    y_tr, y_val = y[train_idx], y[val_idx]\n\n    train_data = lgb.Dataset(X_tr, label=y_tr)\n    val_data = lgb.Dataset(X_val, label=y_val, reference=train_data)\n\n    model = lgb.train(\n        lgb_params_final,\n        train_data,\n        num_boost_round=3500,  # More rounds with lower LR\n        valid_sets=[val_data],\n        callbacks=[lgb.early_stopping(200, verbose=False)],\n    )\n\n    lgb_models_final.append(model)\n    fold_oof = model.predict(X_val)\n    lgb_oof_final_raw[val_idx] = fold_oof\n\n    fold_test = model.predict(X_test)\n    lgb_test_final_raw += fold_test / N_FOLDS\n    lgb_test_folds_final.append(fold_test)\n\n    best_th, best_f1 = 0.5, 0\n    for th in np.arange(0.1, 0.9, 0.01):\n        f1 = f1_score(y_val, (fold_oof >= th).astype(int))\n        if f1 > best_f1:\n            best_f1, best_th = f1, th\n\n    lgb_fold_scores_final.append(best_f1)\n    lgb_fold_thresholds_final.append(best_th)\n    print(f\"Fold {fold}: F1={best_f1:.4f} at threshold={best_th:.2f}\")\n\n# Apply calibration\nprint(\"\\n--- Applying Calibration ---\")\nfrom sklearn.isotonic import IsotonicRegression\n\niso_reg_final = IsotonicRegression(out_of_bounds='clip')\niso_reg_final.fit(lgb_oof_final_raw, y)\n\nlgb_oof_final_cal = iso_reg_final.transform(lgb_oof_final_raw)\nlgb_test_final_cal = iso_reg_final.transform(lgb_test_final_raw)\n\n# Check calibration\nfrom sklearn.calibration import calibration_curve\nfraction_of_positives, mean_predicted_value = calibration_curve(\n    y, lgb_oof_final_cal, n_bins=10, strategy='uniform'\n)\ncal_error_final = np.mean(np.abs(fraction_of_positives - mean_predicted_value))\nprint(f\"   Calibration Error: {cal_error_final:.4f}\")\n\n# Optimize threshold on calibrated predictions\ndef neg_f1_final(th):\n    return -f1_score(y, (lgb_oof_final_cal >= th).astype(int))\n\nresult_final = minimize_scalar(neg_f1_final, bounds=(0.1, 0.9), method=\"bounded\")\nlgb_best_th_final = result_final.x\nlgb_best_f1_final = -result_final.fun\nlgb_auc_final = roc_auc_score(y, lgb_oof_final_cal)\n\nprint(f\"\\n\" + \"=\"*70)\nprint(\"FINAL RESULTS\")\nprint(\"=\"*70)\nprint(f\"\\nLightGBM (Final) CV F1: {np.mean(lgb_fold_scores_final):.4f} ± {np.std(lgb_fold_scores_final):.4f}\")\nprint(f\"LightGBM (Final) Global F1 (calibrated): {lgb_best_f1_final:.4f} at threshold={lgb_best_th_final:.4f}\")\nprint(f\"LightGBM (Final) ROC-AUC: {lgb_auc_final:.4f}\")\nprint(f\"Calibration Error: {cal_error_final:.4f}\")\n\n# Threshold stability comparison\nprint(f\"\\nThreshold Stability:\")\nprint(f\"  Previous std: {np.std(lgb_fold_thresholds_tuned):.4f}\")\nprint(f\"  Final std: {np.std(lgb_fold_thresholds_final):.4f}\")\nthreshold_improvement = np.std(lgb_fold_thresholds_tuned) - np.std(lgb_fold_thresholds_final)\nprint(f\"  Improvement: {threshold_improvement:+.4f}\")\n\n# Compare with all previous models\ncomparisons = []\nif 'lgb_best_f1' in locals():\n    comparisons.append((\"Original\", lgb_best_f1))\nif 'lgb_best_f1_tuned' in locals():\n    comparisons.append((\"Tuned\", lgb_best_f1_tuned))\nif 'lgb_best_f1_finetuned' in locals():\n    comparisons.append((\"Fine-tuned\", lgb_best_f1_finetuned))\n\nif comparisons:\n    print(f\"\\n\" + \"-\"*70)\n    print(\"COMPARISON WITH ALL MODELS\")\n    print(\"-\"*70)\n    for name, score in comparisons:\n        improvement = lgb_best_f1_final - score\n        improvement_pct = improvement / score * 100 if score > 0 else 0\n        print(f\"{name:15s}: F1={score:.4f} → Final F1={lgb_best_f1_final:.4f} ({improvement:+.4f}, {improvement_pct:+.2f}%)\")\n\n# Save everything\nfor i, model in enumerate(lgb_models_final):\n    model.save_model(f\"{GBM_MODEL_DIR}/lgb_model_final_fold{i+1}.txt\")\n\nnp.save(f\"{GBM_MODEL_DIR}/lgb_oof_final_calibrated.npy\", lgb_oof_final_cal)\nnp.save(f\"{GBM_MODEL_DIR}/lgb_test_final_calibrated.npy\", lgb_test_final_cal)\njoblib.dump(iso_reg_final, f\"{GBM_MODEL_DIR}/isotonic_calibration_final.pkl\")\n\n# Final submission with calibrated predictions\nlgb_test_final_sub = lgb_test_final_cal.copy()\nlgb_preds_final = (lgb_test_final_sub >= lgb_best_th_final).astype(int)\n\nsub_lgb_final = pd.DataFrame({\n    \"object_id\": test_log[\"object_id\"],\n    \"target\": lgb_preds_final\n})\nsub_lgb_final.to_csv(\"submission_LGB_final_calibrated.csv\", index=False)\nprint(f\"\\n✅ Saved final calibrated submission -> submission_LGB_final_calibrated.csv\")\n\n# Save config\nlgb_config_final = {\n    \"SEED\": SEED,\n    \"n_features\": X.shape[1],\n    \"scale_pos_weight\": scale_pos_weight,\n    \"lgb_params\": lgb_params_final,\n    \"fold_scores\": lgb_fold_scores_final,\n    \"fold_thresholds\": lgb_fold_thresholds_final,\n    \"threshold_method\": \"calibrated_optimal\",\n    \"best_threshold\": float(lgb_best_th_final),\n    \"best_f1\": float(lgb_best_f1_final),\n    \"roc_auc\": float(lgb_auc_final),\n    \"calibration_error\": float(cal_error_final),\n    \"cv_f1_mean\": float(np.mean(lgb_fold_scores_final)),\n    \"cv_f1_std\": float(np.std(lgb_fold_scores_final)),\n}\njoblib.dump(lgb_config_final, f\"{GBM_MODEL_DIR}/lgb_config_final.pkl\")\n\nprint(\"\\n\" + \"=\"*70)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-21T17:02:19.537800Z","iopub.execute_input":"2025-12-21T17:02:19.538764Z","iopub.status.idle":"2025-12-21T17:02:34.179509Z","shell.execute_reply.started":"2025-12-21T17:02:19.538726Z","shell.execute_reply":"2025-12-21T17:02:34.178534Z"}},"outputs":[{"name":"stdout","text":"\n======================================================================\nFINAL MODEL: CALIBRATION + STRONG REGULARIZATION\n======================================================================\n\n--- Final Hyperparameters (Strong Regularization) ---\n  - num_leaves: 28 → 24\n  - max_depth: 5 → 4\n  - learning_rate: 0.04 → 0.03\n  - feature_fraction: 0.8 → 0.75\n  - min_child_samples: 30 → 40\n  - min_data_in_leaf: 20 → 30\n  - lambda_l1/l2: 0.1 → 0.2\n  - min_gain_to_split: 0.02 → 0.03\n  - max_bin: 255 → 200\n  → Goal: Stable, well-calibrated predictions\n\n--- Training Final Model ---\n\n--- Fold 1/5 ---\nFold 1: F1=0.6197 at threshold=0.26\n\n--- Fold 2/5 ---\nFold 2: F1=0.5714 at threshold=0.32\n\n--- Fold 3/5 ---\nFold 3: F1=0.5455 at threshold=0.40\n\n--- Fold 4/5 ---\nFold 4: F1=0.6792 at threshold=0.49\n\n--- Fold 5/5 ---\nFold 5: F1=0.6102 at threshold=0.49\n\n--- Applying Calibration ---\n   Calibration Error: 0.0000\n\n======================================================================\nFINAL RESULTS\n======================================================================\n\nLightGBM (Final) CV F1: 0.6052 ± 0.0457\nLightGBM (Final) Global F1 (calibrated): 0.5829 at threshold=0.2889\nLightGBM (Final) ROC-AUC: 0.9545\nCalibration Error: 0.0000\n\nThreshold Stability:\n  Previous std: 0.1389\n  Final std: 0.0915\n  Improvement: +0.0473\n\n----------------------------------------------------------------------\nCOMPARISON WITH ALL MODELS\n----------------------------------------------------------------------\nOriginal       : F1=0.5631 → Final F1=0.5829 (+0.0198, +3.51%)\nTuned          : F1=0.5545 → Final F1=0.5829 (+0.0283, +5.11%)\n\n✅ Saved final calibrated submission -> submission_LGB_final_calibrated.csv\n\n======================================================================\n","output_type":"stream"}],"execution_count":39},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}