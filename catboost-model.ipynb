{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-21T16:42:17.172873Z",
     "iopub.status.busy": "2025-12-21T16:42:17.171839Z",
     "iopub.status.idle": "2025-12-21T16:42:27.087960Z",
     "shell.execute_reply": "2025-12-21T16:42:27.086592Z",
     "shell.execute_reply.started": "2025-12-21T16:42:17.172839Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 25.3\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n",
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 25.3\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 24\u001b[39m\n\u001b[32m     21\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpd\u001b[39;00m\n\u001b[32m     22\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mjoblib\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m24\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtensorflow\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtf\u001b[39;00m\n\u001b[32m     25\u001b[39m tf.random.set_seed(SEED)\n\u001b[32m     26\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'tensorflow'"
     ]
    }
   ],
   "source": [
    "!pip -q install -U \"tabpfn == 2.2.1\"\n",
    "!pip -q install pytorch-tabnet\n",
    "\n",
    "# ============================================================\n",
    "# CELL 1: IMPORTS & CONFIGURATION\n",
    "# ============================================================\n",
    "\n",
    "import os, random, gc, warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "SEED = 42\n",
    "os.environ[\"PYTHONHASHSEED\"] = str(SEED)\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"2\"\n",
    "os.environ[\"TF_DETERMINISTIC_OPS\"] = \"1\"\n",
    "\n",
    "random.seed(SEED)\n",
    "\n",
    "import numpy as np\n",
    "np.random.seed(SEED)\n",
    "\n",
    "import pandas as pd\n",
    "import joblib\n",
    "\n",
    "import tensorflow as tf\n",
    "tf.random.set_seed(SEED)\n",
    "try:\n",
    "    tf.config.experimental.enable_op_determinism()\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.layers import (\n",
    "    Input, Dense, GRU, Bidirectional, Layer,\n",
    "    Concatenate, Dropout, SpatialDropout1D, BatchNormalization,\n",
    "    GaussianNoise, Masking\n",
    ")\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold, train_test_split\n",
    "from sklearn.metrics import f1_score, roc_auc_score, classification_report\n",
    "from sklearn.preprocessing import RobustScaler, StandardScaler\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "from scipy.optimize import minimize_scalar\n",
    "\n",
    "import xgboost as xgb\n",
    "from catboost import CatBoostClassifier\n",
    "from tabpfn import TabPFNClassifier\n",
    "from sklearn.ensemble import HistGradientBoostingClassifier\n",
    "from pytorch_tabnet.tab_model import TabNetClassifier\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import optuna\n",
    "from optuna.samplers import TPESampler\n",
    "\n",
    "keras.backend.clear_session()\n",
    "gc.collect()\n",
    "\n",
    "print(\"TF:\", tf.__version__, \"| PyTorch:\", torch.__version__, \"| SEED =\", SEED)\n",
    "\n",
    "DATA_PATH = Path(\"/kaggle/input/project/mallorn-astronomical-classification-challenge\")\n",
    "\n",
    "BANDS = [\"u\", \"g\", \"r\", \"i\", \"z\", \"y\"]\n",
    "BAND_MAP = {b: i for i, b in enumerate(BANDS)}\n",
    "\n",
    "MAX_SEQ_LEN = 300\n",
    "N_FEATURES_PER_STEP = 4\n",
    "\n",
    "MODEL_DIR = \"saved_models\"\n",
    "os.makedirs(MODEL_DIR, exist_ok=True)\n",
    "\n",
    "GBM_MODEL_DIR = \"/kaggle/working/models\"\n",
    "os.makedirs(GBM_MODEL_DIR, exist_ok=True)\n",
    "\n",
    "FEAT_TRAIN_PKL = \"/kaggle/input/2d-gp-features/kaggle/working/cache/train_features_2dgp_gpy.pkl\"\n",
    "FEAT_TEST_PKL  = \"/kaggle/input/2d-gp-features/kaggle/working/cache/test_features_2dgp_gpy.pkl\"\n",
    "\n",
    "N_FOLDS = 5\n",
    "skf = StratifiedKFold(n_splits=N_FOLDS, shuffle=True, random_state=SEED)\n",
    "\n",
    "# SelectKBest configuration (cho cả NN meta và GBM)\n",
    "USE_SELECTKBEST = False   # Set False to disable feature selection\n",
    "K_BEST_RATIO = 0.8       # Keep 80% of features (or use absolute number if < 1)\n",
    "K_BEST_MIN = 200         # Minimum number of features to keep\n",
    "K_BEST_MAX = None        # Maximum number of features (None = no limit)\n",
    "\n",
    "# Device for PyTorch models\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f\"PyTorch models will use device: {DEVICE}\")\n",
    "\n",
    "print(\"Config done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-21T16:43:09.050200Z",
     "iopub.status.busy": "2025-12-21T16:43:09.049809Z",
     "iopub.status.idle": "2025-12-21T16:43:10.437155Z",
     "shell.execute_reply": "2025-12-21T16:43:10.436192Z",
     "shell.execute_reply.started": "2025-12-21T16:43:09.050169Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Loading Data ---\n",
      "Train LC: (479384, 5), Test LC: (1145125, 5)\n",
      "Train objects: 3043, Test objects: 7135\n",
      "TDE count: 148, TDE ratio: 4.86%\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# CELL 2: LOAD DATA\n",
    "# ============================================================\n",
    "\n",
    "print(\"\\n--- Loading Data ---\")\n",
    "\n",
    "train_log = pd.read_csv(DATA_PATH / \"train_log.csv\")\n",
    "test_log  = pd.read_csv(DATA_PATH / \"test_log.csv\")\n",
    "\n",
    "def load_lc(split, kind):\n",
    "    return pd.read_csv(DATA_PATH / split / f\"{kind}_full_lightcurves.csv\")\n",
    "\n",
    "train_lc = pd.concat([load_lc(s, \"train\") for s in train_log[\"split\"].unique()], ignore_index=True)\n",
    "test_lc  = pd.concat([load_lc(s, \"test\")  for s in test_log[\"split\"].unique()],  ignore_index=True)\n",
    "\n",
    "train_feat = pd.read_pickle(FEAT_TRAIN_PKL)\n",
    "test_feat  = pd.read_pickle(FEAT_TEST_PKL)\n",
    "\n",
    "y = train_log[\"target\"].values.astype(np.int32)\n",
    "\n",
    "print(f\"Train LC: {train_lc.shape}, Test LC: {test_lc.shape}\")\n",
    "print(f\"Train objects: {len(train_log)}, Test objects: {len(test_log)}\")\n",
    "print(f\"TDE count: {train_log['target'].sum()}, TDE ratio: {train_log['target'].mean()*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-21T16:46:05.196107Z",
     "iopub.status.busy": "2025-12-21T16:46:05.195734Z",
     "iopub.status.idle": "2025-12-21T16:46:05.621934Z",
     "shell.execute_reply": "2025-12-21T16:46:05.620882Z",
     "shell.execute_reply.started": "2025-12-21T16:46:05.196079Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "ANALYSIS 1: FEATURE STATISTICS\n",
      "======================================================================\n",
      "\n",
      "1. NaN/Inf Analysis:\n",
      "\n",
      "  Total features: 308\n",
      "  Features with >50% NaN: 0\n",
      "\n",
      "2. Feature Scale Analysis:\n",
      "  Mean std across features: 3663.5524\n",
      "  Max std: 1097813.4700\n",
      "  Min std: 0.0000\n",
      "\n",
      "  Features with VERY HIGH std (>1000): 7\n",
      "               feature          std           min          max\n",
      "         gp2d_ls_ratio 1.097813e+06  6.153001e-08 4.299914e+07\n",
      "u_gp2d_integrated_flux 2.955390e+03 -3.252791e+04 8.786704e+04\n",
      "g_gp2d_integrated_flux 2.624058e+03 -2.264464e+04 6.584307e+04\n",
      "r_gp2d_integrated_flux 2.447970e+03 -1.457523e+04 4.652942e+04\n",
      "i_gp2d_integrated_flux 2.395894e+03 -1.254920e+04 4.101660e+04\n",
      "y_gp2d_integrated_flux 2.349733e+03 -1.171562e+04 3.981636e+04\n",
      "z_gp2d_integrated_flux 2.348133e+03 -1.210663e+04 4.057562e+04\n",
      "\n",
      "  Features with ZERO/NEAR-ZERO variance (<1e-6): 5\n",
      "     feature  std\n",
      "       Z_err  0.0\n",
      "count_snr_-3  0.0\n",
      "count_snr_-5  0.0\n",
      " frac_snr_-3  0.0\n",
      " frac_snr_-5  0.0\n",
      "\n",
      "3. Feature Distribution Skewness:\n",
      "  Features with HIGH skewness (|skew| > 5): 119\n",
      "                  feature  skewness         mean          std\n",
      "y_gp2d_rise_decline_ratio 47.172489     3.224239 4.102087e+01\n",
      "   gp2d_length_scale_wave 40.187550    68.874448 8.140546e+02\n",
      "u_gp2d_rise_decline_ratio 33.811164     2.679148 2.739702e+01\n",
      "            gp2d_ls_ratio 33.410622 36393.851094 1.097813e+06\n",
      "   gp2d_length_scale_time 32.430631     0.150614 1.153628e+00\n",
      "      gp2d_peak_ratio_z_i 30.970659     0.990096 7.777007e-01\n",
      "         r_gp2d_rise_rate 27.513628     0.004800 1.518006e-02\n",
      "         i_gp2d_rise_rate 26.551997     0.005117 1.581873e-02\n",
      "g_gp2d_rise_decline_ratio 26.366216     2.462979 1.683355e+01\n",
      "i_gp2d_rise_decline_ratio 25.959848     2.138108 1.061683e+01\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# CELL 9: FEATURE STATISTICS ANALYSIS\n",
    "# ============================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"ANALYSIS 1: FEATURE STATISTICS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Analyze original features before scaling\n",
    "print(\"\\n1. NaN/Inf Analysis:\")\n",
    "nan_counts = pd.DataFrame({\n",
    "    'feature': feature_names,\n",
    "    'nan_count': [np.isnan(X_raw[:, i]).sum() for i in range(X_raw.shape[1])],\n",
    "    'inf_count': [np.isinf(X_raw[:, i]).sum() for i in range(X_raw.shape[1])],\n",
    "    'zero_count': [(X_raw[:, i] == 0).sum() for i in range(X_raw.shape[1])],\n",
    "})\n",
    "\n",
    "nan_counts['nan_pct'] = (nan_counts['nan_count'] / len(X_raw) * 100).round(2)\n",
    "nan_counts['zero_pct'] = (nan_counts['zero_count'] / len(X_raw) * 100).round(2)\n",
    "\n",
    "high_nan = nan_counts[nan_counts['nan_pct'] > 50].sort_values('nan_pct', ascending=False)\n",
    "print(f\"\\n  Total features: {len(nan_counts)}\")\n",
    "print(f\"  Features with >50% NaN: {len(high_nan)}\")\n",
    "if len(high_nan) > 0:\n",
    "    print(f\"  Top 10 features with most NaN:\")\n",
    "    print(high_nan.head(10)[['feature', 'nan_pct', 'zero_pct']].to_string(index=False))\n",
    "\n",
    "print(\"\\n2. Feature Scale Analysis:\")\n",
    "feature_stats = pd.DataFrame({\n",
    "    'feature': feature_names,\n",
    "    'mean': [np.nanmean(X_raw[:, i]) for i in range(X_raw.shape[1])],\n",
    "    'std': [np.nanstd(X_raw[:, i]) for i in range(X_raw.shape[1])],\n",
    "    'min': [np.nanmin(X_raw[:, i]) for i in range(X_raw.shape[1])],\n",
    "    'max': [np.nanmax(X_raw[:, i]) for i in range(X_raw.shape[1])],\n",
    "    'q25': [np.nanpercentile(X_raw[:, i], 25) for i in range(X_raw.shape[1])],\n",
    "    'q75': [np.nanpercentile(X_raw[:, i], 75) for i in range(X_raw.shape[1])],\n",
    "})\n",
    "\n",
    "# Remove NaN/inf for analysis\n",
    "feature_stats = feature_stats.replace([np.inf, -np.inf], np.nan)\n",
    "\n",
    "print(f\"  Mean std across features: {feature_stats['std'].mean():.4f}\")\n",
    "print(f\"  Max std: {feature_stats['std'].max():.4f}\")\n",
    "print(f\"  Min std: {feature_stats['std'].min():.4f}\")\n",
    "\n",
    "# Features with extreme scales\n",
    "extreme_std = feature_stats[feature_stats['std'] > 1000].sort_values('std', ascending=False)\n",
    "if len(extreme_std) > 0:\n",
    "    print(f\"\\n  Features with VERY HIGH std (>1000): {len(extreme_std)}\")\n",
    "    print(extreme_std.head(10)[['feature', 'std', 'min', 'max']].to_string(index=False))\n",
    "\n",
    "zero_var = feature_stats[feature_stats['std'] < 1e-6]\n",
    "if len(zero_var) > 0:\n",
    "    print(f\"\\n  Features with ZERO/NEAR-ZERO variance (<1e-6): {len(zero_var)}\")\n",
    "    print(zero_var[['feature', 'std']].head(20).to_string(index=False))\n",
    "\n",
    "print(\"\\n3. Feature Distribution Skewness:\")\n",
    "from scipy.stats import skew\n",
    "feature_stats['skewness'] = [skew(X_raw[:, i]) if np.nanstd(X_raw[:, i]) > 1e-6 else 0 \n",
    "                             for i in range(X_raw.shape[1])]\n",
    "high_skew = feature_stats[feature_stats['skewness'].abs() > 5].sort_values('skewness', key=abs, ascending=False)\n",
    "if len(high_skew) > 0:\n",
    "    print(f\"  Features with HIGH skewness (|skew| > 5): {len(high_skew)}\")\n",
    "    print(high_skew.head(10)[['feature', 'skewness', 'mean', 'std']].to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "=> Apply Select Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-21T16:46:10.020621Z",
     "iopub.status.busy": "2025-12-21T16:46:10.020269Z",
     "iopub.status.idle": "2025-12-21T16:46:10.117378Z",
     "shell.execute_reply": "2025-12-21T16:46:10.116429Z",
     "shell.execute_reply.started": "2025-12-21T16:46:10.020593Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "LOADING DATA (REUSED IN-MEMORY) FOR GBM\n",
      "============================================================\n",
      "Train features: (3043, 309)\n",
      "Test features: (7135, 309)\n",
      "TDE count: 148, TDE ratio: 4.86%\n",
      "\n",
      "--- Preparing GBM Features ---\n",
      "Original number of GBM features: 308\n",
      "\n",
      "Applying SelectKBest (GBM): 308 -> 246 features\n",
      "  (ratio=0.8, min=200, max=None)\n",
      "Selected 246 features for GBM\n",
      "✅ Saved GBM feature selector and selected feature names\n",
      "\n",
      "Final GBM feature count: 246\n",
      "Final GBM train shape: (3043, 246), Final GBM test shape: (7135, 246)\n",
      "Scale pos weight: 19.56\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# CELL 7: PREPARE FEATURES WITH SelectKBest (FOR GBM MODELS)\n",
    "# ============================================================\n",
    "USE_SELECTKBEST = True\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"LOADING DATA (REUSED IN-MEMORY) FOR GBM\")\n",
    "print(\"=\"*60)\n",
    "K_BEST_RATIO = 0.8\n",
    "X_df = train_feat.copy()\n",
    "X_test_df = test_feat.copy()\n",
    "\n",
    "print(f\"Train features: {X_df.shape}\")\n",
    "print(f\"Test features: {X_test_df.shape}\")\n",
    "print(f\"TDE count: {y.sum()}, TDE ratio: {y.mean()*100:.2f}%\")\n",
    "\n",
    "print(\"\\n--- Preparing GBM Features ---\")\n",
    "\n",
    "if \"object_id\" in X_df.columns:\n",
    "    X_df = X_df.drop(columns=[\"object_id\"])\n",
    "if \"object_id\" in X_test_df.columns:\n",
    "    X_test_df = X_test_df.drop(columns=[\"object_id\"])\n",
    "\n",
    "# Deterministic order\n",
    "common_cols = sorted(set(X_df.columns) & set(X_test_df.columns))\n",
    "X_df = X_df[common_cols]\n",
    "X_test_df = X_test_df[common_cols]\n",
    "\n",
    "X_df = X_df.replace([np.inf, -np.inf], np.nan).fillna(0)\n",
    "X_test_df = X_test_df.replace([np.inf, -np.inf], np.nan).fillna(0)\n",
    "\n",
    "feature_names = X_df.columns.tolist()\n",
    "X_raw = X_df.values\n",
    "X_test_raw = X_test_df.values\n",
    "\n",
    "n_features_original = X_raw.shape[1]\n",
    "print(f\"Original number of GBM features: {n_features_original}\")\n",
    "\n",
    "feature_selector = None\n",
    "selected_feature_names = feature_names\n",
    "\n",
    "os.makedirs(GBM_MODEL_DIR, exist_ok=True)\n",
    "\n",
    "if USE_SELECTKBEST and n_features_original > K_BEST_MIN:\n",
    "    # Calculate k_best\n",
    "    if K_BEST_RATIO < 1.0:\n",
    "        k_best = int(n_features_original * K_BEST_RATIO)\n",
    "    else:\n",
    "        k_best = int(K_BEST_RATIO)\n",
    "    \n",
    "    # Apply min/max constraints\n",
    "    k_best = max(K_BEST_MIN, k_best)\n",
    "    if K_BEST_MAX is not None:\n",
    "        k_best = min(K_BEST_MAX, k_best)\n",
    "    k_best = min(k_best, n_features_original)\n",
    "    \n",
    "    print(f\"\\nApplying SelectKBest (GBM): {n_features_original} -> {k_best} features\")\n",
    "    print(f\"  (ratio={K_BEST_RATIO}, min={K_BEST_MIN}, max={K_BEST_MAX})\")\n",
    "    \n",
    "    # Fit SelectKBest on full training data\n",
    "    feature_selector = SelectKBest(score_func=f_classif, k=k_best)\n",
    "    X = feature_selector.fit_transform(X_raw, y)\n",
    "    X_test = feature_selector.transform(X_test_raw)\n",
    "    \n",
    "    # Get selected feature names\n",
    "    selected_mask = feature_selector.get_support()\n",
    "    selected_feature_names = [feature_names[i] for i in range(len(feature_names)) if selected_mask[i]]\n",
    "    \n",
    "    print(f\"Selected {len(selected_feature_names)} features for GBM\")\n",
    "    \n",
    "    # Save selector\n",
    "    joblib.dump(feature_selector, f\"{GBM_MODEL_DIR}/gbm_feature_selector.pkl\")\n",
    "    joblib.dump(selected_feature_names, f\"{GBM_MODEL_DIR}/gbm_selected_feature_names.pkl\")\n",
    "    print(f\"✅ Saved GBM feature selector and selected feature names\")\n",
    "    \n",
    "else:\n",
    "    print(f\"\\nSkipping SelectKBest for GBM (using all {n_features_original} features)\")\n",
    "    X = X_raw\n",
    "    X_test = X_test_raw\n",
    "    \n",
    "    # Save all feature names\n",
    "    joblib.dump(selected_feature_names, f\"{GBM_MODEL_DIR}/gbm_all_feature_names.pkl\")\n",
    "\n",
    "print(f\"\\nFinal GBM feature count: {X.shape[1]}\")\n",
    "print(f\"Final GBM train shape: {X.shape}, Final GBM test shape: {X_test.shape}\")\n",
    "\n",
    "scale_pos_weight = (y == 0).sum() / (y == 1).sum()\n",
    "print(f\"Scale pos weight: {scale_pos_weight:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-21T16:46:11.663786Z",
     "iopub.status.busy": "2025-12-21T16:46:11.662910Z",
     "iopub.status.idle": "2025-12-21T16:46:24.439817Z",
     "shell.execute_reply": "2025-12-21T16:46:24.438746Z",
     "shell.execute_reply.started": "2025-12-21T16:46:11.663752Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "TRAINING LIGHTGBM\n",
      "============================================================\n",
      "\n",
      "--- Fold 1/5 ---\n",
      "Fold 1: F1=0.5263 at threshold=0.19\n",
      "\n",
      "--- Fold 2/5 ---\n",
      "Fold 2: F1=0.6250 at threshold=0.30\n",
      "\n",
      "--- Fold 3/5 ---\n",
      "Fold 3: F1=0.5067 at threshold=0.12\n",
      "\n",
      "--- Fold 4/5 ---\n",
      "Fold 4: F1=0.6667 at threshold=0.34\n",
      "\n",
      "--- Fold 5/5 ---\n",
      "Fold 5: F1=0.5758 at threshold=0.35\n",
      "\n",
      "LightGBM CV F1: 0.5801 ± 0.0597\n",
      "LightGBM Global F1: 0.5631 at threshold=0.34\n",
      "LightGBM ROC-AUC: 0.9425\n",
      "\n",
      "Saved single-fold LGB submission (best fold #4) -> submission_single_fold_LGB.csv\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# CELL 8: TRAIN CATBOOST (WITH BEST-FOLD SUBMISSION)\n",
    "# ============================================================\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import joblib\n",
    "from itertools import combinations\n",
    "from scipy.stats import rankdata\n",
    "from scipy.optimize import minimize_scalar\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"TRAINING CATBOOST\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "cb_params = {\n",
    "    \"loss_function\": \"Logloss\",\n",
    "    \"eval_metric\": \"AUC\",\n",
    "    \"depth\": 8,\n",
    "    \"learning_rate\": 0.05,\n",
    "    \"iterations\": 2000,\n",
    "    \"l2_leaf_reg\": 3.0,\n",
    "    \"random_seed\": SEED,\n",
    "    \"bootstrap_type\": \"Bernoulli\",\n",
    "    \"subsample\": 0.8,\n",
    "    \"colsample_bylevel\": 0.8,\n",
    "    \"min_data_in_leaf\": 20,\n",
    "    \"class_weights\": [1.0, scale_pos_weight],\n",
    "    \"task_type\": \"CPU\",\n",
    "    \"verbose\": False,\n",
    "}\n",
    "\n",
    "cb_oof = np.zeros(len(y))\n",
    "cb_test = np.zeros(len(X_test))\n",
    "cb_models = []\n",
    "cb_fold_scores = []\n",
    "cb_fold_thresholds = []\n",
    "cb_test_folds = []\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(skf.split(X, y), 1):\n",
    "    print(f\"\\n--- Fold {fold}/{N_FOLDS} ---\")\n",
    "\n",
    "    X_tr, X_val = X[train_idx], X[val_idx]\n",
    "    y_tr, y_val = y[train_idx], y[val_idx]\n",
    "\n",
    "    model = CatBoostClassifier(**cb_params)\n",
    "    model.fit(\n",
    "        X_tr, y_tr,\n",
    "        eval_set=(X_val, y_val),\n",
    "        use_best_model=True,\n",
    "        early_stopping_rounds=200,\n",
    "        verbose=False,\n",
    "    )\n",
    "\n",
    "    cb_models.append(model)\n",
    "    fold_oof = model.predict_proba(X_val)[:, 1]\n",
    "    cb_oof[val_idx] = fold_oof\n",
    "\n",
    "    fold_test = model.predict_proba(X_test)[:, 1]\n",
    "    cb_test += fold_test / N_FOLDS\n",
    "    cb_test_folds.append(fold_test)\n",
    "\n",
    "    best_th, best_f1 = 0.5, 0\n",
    "    for th in np.arange(0.1, 0.9, 0.01):\n",
    "        f1 = f1_score(y_val, (fold_oof >= th).astype(int))\n",
    "        if f1 > best_f1:\n",
    "            best_f1, best_th = f1, th\n",
    "\n",
    "    cb_fold_scores.append(best_f1)\n",
    "    cb_fold_thresholds.append(best_th)\n",
    "    print(f\"Fold {fold}: F1={best_f1:.4f} at threshold={best_th:.2f}\")\n",
    "\n",
    "def neg_f1(th):\n",
    "    return -f1_score(y, (cb_oof >= th).astype(int))\n",
    "\n",
    "result = minimize_scalar(neg_f1, bounds=(0.1, 0.9), method=\"bounded\")\n",
    "cb_best_th = result.x\n",
    "cb_best_f1 = -result.fun\n",
    "cb_auc = roc_auc_score(y, cb_oof)\n",
    "\n",
    "print(f\"\\nCatBoost CV F1: {np.mean(cb_fold_scores):.4f} ± {np.std(cb_fold_scores):.4f}\")\n",
    "print(f\"CatBoost Global F1: {cb_best_f1:.4f} at threshold={cb_best_th:.2f}\")\n",
    "print(f\"CatBoost ROC-AUC: {cb_auc:.4f}\")\n",
    "\n",
    "# Submission từ fold có F1 cao nhất\n",
    "best_cb_fold = int(np.argmax(cb_fold_scores))\n",
    "best_cb_th = float(cb_fold_thresholds[best_cb_fold])\n",
    "best_cb_test = np.asarray(cb_test_folds[best_cb_fold], dtype=float)\n",
    "\n",
    "cb_single_preds = (best_cb_test >= best_cb_th).astype(int)\n",
    "sub_cb_single = pd.DataFrame({\n",
    "    \"object_id\": test_log[\"object_id\"],\n",
    "    \"target\": cb_single_preds\n",
    "})\n",
    "sub_cb_single.to_csv(\"submission_single_fold_CAT.csv\", index=False)\n",
    "print(f\"\\nSaved single-fold CatBoost submission (best fold #{best_cb_fold+1}) -> submission_single_fold_CAT.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-21T16:50:36.009614Z",
     "iopub.status.busy": "2025-12-21T16:50:36.009282Z",
     "iopub.status.idle": "2025-12-21T16:50:38.237913Z",
     "shell.execute_reply": "2025-12-21T16:50:38.236936Z",
     "shell.execute_reply.started": "2025-12-21T16:50:36.009579Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "ANALYSIS: SHAP VALUES FOR FEATURE IMPORTANCE\n",
      "======================================================================\n",
      "✅ SHAP library available\n",
      "\n",
      "Using sample size: 3043 for SHAP calculation\n",
      "(For full dataset, increase SAMPLE_SIZE or use tree explainer)\n",
      "\n",
      "Using model from fold 4 (best F1: 0.6667)\n",
      "\n",
      "Calculating SHAP values...\n",
      "✅ SHAP values calculated: shape (3043, 246)\n",
      "\n",
      "======================================================================\n",
      "1. GLOBAL FEATURE IMPORTANCE (SHAP)\n",
      "======================================================================\n",
      "\n",
      "Top 30 Features by SHAP Importance:\n",
      "                feature  shap_importance\n",
      "i_gp2d_peaks_pos_frac_2         0.488558\n",
      "    i_gp2d_time_fwd_0.5         0.420239\n",
      "              i_snr_max         0.355115\n",
      "    i_gp2d_decline_rate         0.312741\n",
      "           flux_std_all         0.203499\n",
      "         flux_range_all         0.187759\n",
      "           flux_max_all         0.181696\n",
      "    r_gp2d_time_bwd_0.8         0.167534\n",
      "      count_max_fall_30         0.159222\n",
      "             r_flux_std         0.157541\n",
      "             i_flux_min         0.151830\n",
      "    g_gp2d_time_fwd_0.5         0.148489\n",
      "  i_gp2d_positive_width         0.137879\n",
      "               g_pct_25         0.131938\n",
      "       g_gp2d_peak_time         0.131741\n",
      "    i_gp2d_time_fwd_0.8         0.130865\n",
      "                      Z         0.129377\n",
      "            g_gp2d_fwhm         0.125259\n",
      "               g_pct_50         0.124717\n",
      "             r_flux_max         0.123002\n",
      "               g_pct_75         0.122575\n",
      "    r_gp2d_time_bwd_0.2         0.102583\n",
      "             i_snr_mean         0.098154\n",
      "               i_pct_75         0.091270\n",
      "        g_gp2d_mean_std         0.085867\n",
      "gp2d_spectral_evolution         0.080210\n",
      "        u_gp2d_abs_diff         0.079229\n",
      "             g_flux_max         0.079205\n",
      "       r_gp2d_peak_time         0.074918\n",
      "    gp2d_blue_red_ratio         0.074556\n",
      "\n",
      "Bottom 20 Features by SHAP Importance (potentially useless):\n",
      "                  feature  shap_importance\n",
      "               snr_median         0.000488\n",
      "   r_gp2d_peaks_pos_count         0.000467\n",
      "    i_gp2d_negative_width         0.000399\n",
      "i_gp2d_rise_decline_ratio         0.000366\n",
      "              y_flux_mean         0.000366\n",
      "              u_gp2d_fwhm         0.000200\n",
      "         gp2d_peak_dt_i_z         0.000183\n",
      "                r_snr_max         0.000158\n",
      "              frac_snr_10         0.000142\n",
      "           u_gp2d_min_std         0.000137\n",
      "         gp2d_peak_dt_g_z         0.000091\n",
      "           g_gp2d_max_std         0.000062\n",
      "     gp2d_kernel_variance         0.000000\n",
      "           gp2d_color_r_i         0.000000\n",
      "              frac_snr_20         0.000000\n",
      "    r_gp2d_positive_width         0.000000\n",
      "  i_gp2d_peaks_neg_frac_2         0.000000\n",
      "  r_gp2d_peaks_pos_frac_3         0.000000\n",
      "   r_gp2d_integrated_flux         0.000000\n",
      "           r_gp2d_max_std         0.000000\n",
      "\n",
      "======================================================================\n",
      "2. FEATURE IMPACT BY CLASS (TDE vs Non-TDE)\n",
      "======================================================================\n",
      "\n",
      "TDE samples: 148, Non-TDE samples: 2895\n",
      "\n",
      "Top 20 Features MORE Important for TDE predictions:\n",
      "                feature  shap_TDE  shap_NonTDE  shap_diff\n",
      "    i_gp2d_decline_rate  0.537583     0.301246   0.236337\n",
      "         flux_range_all  0.371728     0.178354   0.193374\n",
      "              i_snr_max  0.507508     0.347324   0.160184\n",
      "    i_gp2d_time_fwd_0.5  0.559091     0.413140   0.145951\n",
      "    i_gp2d_time_fwd_0.8  0.254815     0.124528   0.130288\n",
      "    r_gp2d_time_bwd_0.8  0.281284     0.161719   0.119566\n",
      "             r_flux_std  0.267975     0.151895   0.116080\n",
      "                      Z  0.224463     0.124516   0.099946\n",
      "               i_pct_75  0.178554     0.086808   0.091746\n",
      "               g_pct_75  0.209463     0.118133   0.091330\n",
      "            g_gp2d_fwhm  0.211939     0.120828   0.091111\n",
      "i_gp2d_peaks_pos_frac_2  0.572791     0.484252   0.088539\n",
      "               g_pct_25  0.209010     0.127998   0.081012\n",
      "    g_gp2d_time_fwd_0.5  0.212020     0.145241   0.066779\n",
      "      count_max_fall_30  0.219141     0.156159   0.062982\n",
      "               g_pct_50  0.170777     0.122362   0.048415\n",
      "    r_gp2d_time_fwd_0.8  0.096256     0.048029   0.048227\n",
      "           flux_max_all  0.225585     0.179453   0.046132\n",
      "              color_g_i  0.086595     0.040992   0.045603\n",
      "    r_gp2d_time_bwd_0.2  0.143708     0.100481   0.043227\n",
      "\n",
      "Top 20 Features MORE Important for Non-TDE predictions:\n",
      "                  feature  shap_TDE  shap_NonTDE     shap_diff\n",
      "              frac_snr_20  0.000000     0.000000  0.000000e+00\n",
      "           r_gp2d_max_std  0.000000     0.000000  0.000000e+00\n",
      "    r_gp2d_positive_width  0.000000     0.000000  0.000000e+00\n",
      "                r_snr_max  0.000157     0.000158 -9.893809e-07\n",
      "           g_gp2d_max_std  0.000042     0.000063 -2.087259e-05\n",
      "      u_gp2d_time_bwd_0.8  0.001000     0.001024 -2.440747e-05\n",
      "         gp2d_peak_dt_g_z  0.000042     0.000093 -5.169133e-05\n",
      "         gp2d_peak_dt_i_z  0.000131     0.000186 -5.480836e-05\n",
      "      i_gp2d_time_bwd_0.5  0.000620     0.000714 -9.417008e-05\n",
      "   r_gp2d_peaks_pos_count  0.000339     0.000474 -1.355052e-04\n",
      "i_gp2d_rise_decline_ratio  0.000236     0.000372 -1.362020e-04\n",
      "   gp2d_length_scale_time  0.001406     0.001675 -2.686243e-04\n",
      "   g_gp2d_integrated_flux  0.001111     0.001490 -3.793518e-04\n",
      "   i_gp2d_integrated_flux  0.001743     0.002208 -4.648110e-04\n",
      "               i_flux_max  0.052488     0.053016 -5.274592e-04\n",
      "         gp2d_peak_dt_u_g  0.002855     0.003402 -5.473413e-04\n",
      "              r_total_snr  0.006275     0.006953 -6.785851e-04\n",
      "                  g_n_obs  0.005105     0.006493 -1.387560e-03\n",
      "   u_gp2d_integrated_flux  0.001018     0.002472 -1.454315e-03\n",
      "      g_gp2d_time_fwd_0.8  0.012686     0.015529 -2.842486e-03\n",
      "\n",
      "   Found 78 features with CLASS-SPECIFIC importance:\n",
      "                feature  shap_TDE  shap_NonTDE  shap_diff\n",
      "    i_gp2d_decline_rate  0.537583     0.301246   0.236337\n",
      "         flux_range_all  0.371728     0.178354   0.193374\n",
      "              i_snr_max  0.507508     0.347324   0.160184\n",
      "    i_gp2d_time_fwd_0.5  0.559091     0.413140   0.145951\n",
      "    i_gp2d_time_fwd_0.8  0.254815     0.124528   0.130288\n",
      "    r_gp2d_time_bwd_0.8  0.281284     0.161719   0.119566\n",
      "             r_flux_std  0.267975     0.151895   0.116080\n",
      "                      Z  0.224463     0.124516   0.099946\n",
      "               i_pct_75  0.178554     0.086808   0.091746\n",
      "               g_pct_75  0.209463     0.118133   0.091330\n",
      "            g_gp2d_fwhm  0.211939     0.120828   0.091111\n",
      "i_gp2d_peaks_pos_frac_2  0.572791     0.484252   0.088539\n",
      "               g_pct_25  0.209010     0.127998   0.081012\n",
      "    g_gp2d_time_fwd_0.5  0.212020     0.145241   0.066779\n",
      "      count_max_fall_30  0.219141     0.156159   0.062982\n",
      "   → These features behave differently for TDE vs Non-TDE\n",
      "\n",
      "======================================================================\n",
      "3. MISCLASSIFICATION ANALYSIS WITH SHAP\n",
      "======================================================================\n",
      "\n",
      "FP (False Positives): 11\n",
      "FN (False Negatives): 9\n",
      "TP (True Positives): 139\n",
      "TN (True Negatives): 2884\n",
      "\n",
      "   Top 15 Features causing False Positives (confusing Non-TDE as TDE):\n",
      "                feature  fp_shap  tn_shap     diff\n",
      "    i_gp2d_time_fwd_0.5 0.539579 0.258852 0.280727\n",
      "    i_gp2d_decline_rate 0.555728 0.316977 0.238751\n",
      "         flux_range_all 0.380163 0.203913 0.176250\n",
      "        u_gp2d_abs_diff 0.210015 0.043557 0.166458\n",
      "    r_gp2d_time_bwd_0.8 0.305047 0.168730 0.136317\n",
      "              i_snr_max 0.435016 0.327889 0.107126\n",
      "               g_pct_75 0.169764 0.084790 0.084974\n",
      "            g_gp2d_fwhm 0.218408 0.139516 0.078892\n",
      "               g_pct_25 0.184852 0.116457 0.068395\n",
      "             r_flux_std 0.239806 0.172236 0.067570\n",
      "               i_pct_75 0.134324 0.068482 0.065842\n",
      "    r_gp2d_time_bwd_0.2 0.164509 0.104857 0.059652\n",
      "              color_g_i 0.108476 0.052843 0.055633\n",
      "    i_gp2d_time_fwd_0.8 0.197077 0.148051 0.049026\n",
      "u_gp2d_peaks_neg_frac_2 0.088906 0.040142 0.048764\n",
      "\n",
      "   Top 15 Features causing False Negatives (missing TDE):\n",
      "                feature  fn_shap  tp_shap     diff\n",
      "        u_gp2d_abs_diff 0.134089 0.072137 0.061952\n",
      "             i_snr_mean 0.096496 0.052401 0.044095\n",
      "                      Z 0.266210 0.223926 0.042284\n",
      "u_gp2d_peaks_pos_frac_2 0.067853 0.039202 0.028650\n",
      "       gp2d_peak_dt_u_r 0.054604 0.028558 0.026046\n",
      "           count_snr_-5 0.054051 0.028082 0.025969\n",
      "               g_pct_50 0.210392 0.187972 0.022420\n",
      "  y_gp2d_negative_width 0.061859 0.039505 0.022354\n",
      "       r_gp2d_peak_flux 0.056660 0.035059 0.021601\n",
      "            i_flux_mean 0.087951 0.066797 0.021154\n",
      "                i_n_obs 0.036472 0.015861 0.020612\n",
      "    r_gp2d_time_bwd_0.8 0.257927 0.238736 0.019191\n",
      "g_gp2d_peaks_neg_frac_2 0.056291 0.037600 0.018691\n",
      "         y_gp2d_min_std 0.055839 0.038099 0.017740\n",
      "       g_gp2d_peak_flux 0.046894 0.030306 0.016587\n",
      "\n",
      "======================================================================\n",
      "4. FEATURE VALUES THAT CAUSE CONFUSION\n",
      "======================================================================\n",
      "\n",
      "   Features with DIFFERENT SHAP values for misclassified vs correctly classified:\n",
      "                feature  misclassified_shap  correct_shap  shap_diff\n",
      "i_gp2d_peaks_pos_frac_2            0.521968     -0.026776   0.548744\n",
      "    i_gp2d_time_fwd_0.5            0.505722     -0.004082   0.509804\n",
      "    i_gp2d_decline_rate            0.378190     -0.077304   0.455494\n",
      "              i_snr_max            0.409852     -0.014491   0.424344\n",
      "         flux_range_all            0.315242      0.024871   0.290371\n",
      "    i_gp2d_time_fwd_0.8            0.176599      0.022771   0.153828\n",
      "  i_gp2d_positive_width            0.149826     -0.000881   0.150706\n",
      "      count_max_fall_30            0.167974      0.017855   0.150119\n",
      "             r_flux_std            0.136990     -0.005976   0.142965\n",
      "             i_flux_min            0.134683     -0.005685   0.140369\n",
      "    r_gp2d_time_bwd_0.8            0.130656      0.003248   0.127408\n",
      "            g_gp2d_fwhm            0.114245      0.000129   0.114116\n",
      "    r_gp2d_time_bwd_0.2            0.106953      0.004217   0.102736\n",
      "             r_flux_max            0.085840     -0.010536   0.096376\n",
      "               g_pct_75            0.095063      0.004399   0.090665\n",
      "           flux_std_all            0.088935      0.001964   0.086971\n",
      "           flux_max_all            0.076584     -0.006720   0.083304\n",
      "        g_gp2d_mean_std            0.086104      0.003674   0.082430\n",
      "gp2d_spectral_evolution            0.083860      0.001492   0.082369\n",
      "             g_flux_max            0.085046      0.003431   0.081615\n",
      "\n",
      "======================================================================\n",
      "KEY INSIGHTS FROM SHAP\n",
      "======================================================================\n",
      "\n",
      "\n",
      "1. ⚠️  78 features have class-specific importance\n",
      "2.    → Consider class-specific feature engineering or weighted features\n",
      "3. ⚠️  Misclassifications found: 11 FP, 9 FN\n",
      "4.    → Check feature values and interactions for these cases\n",
      "\n",
      "======================================================================\n",
      "\n",
      "✅ Saved SHAP importance to /kaggle/working/models/shap_importance.csv\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# CELL 18: SHAP ANALYSIS FOR WEAKNESS DETECTION\n",
    "# ============================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"ANALYSIS: SHAP VALUES FOR FEATURE IMPORTANCE (CatBoost)\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "try:\n",
    "    import shap\n",
    "    print(\"✅ SHAP library available\\n\")\n",
    "except ImportError:\n",
    "    print(\"❌ Installing SHAP...\")\n",
    "    !pip install -q shap\n",
    "    import shap\n",
    "    print(\"✅ SHAP installed\\n\")\n",
    "\n",
    "# Use a sample for SHAP (tính toán nhanh hơn)\n",
    "# Hoặc dùng toàn bộ data nếu có đủ thời gian\n",
    "SAMPLE_SIZE = min(10000, len(X))  # Sample size for SHAP calculation\n",
    "\n",
    "print(f\"Using sample size: {SAMPLE_SIZE} for SHAP calculation\")\n",
    "print(\"(For full dataset, increase SAMPLE_SIZE or use tree explainer)\\n\")\n",
    "\n",
    "# Sample data\n",
    "sample_idx = np.random.choice(len(X), size=SAMPLE_SIZE, replace=False)\n",
    "X_sample = X[sample_idx]\n",
    "y_sample = y[sample_idx]\n",
    "\n",
    "# Use one of the trained CatBoost models (best fold)\n",
    "best_model_idx = np.argmax(cb_fold_scores)\n",
    "shap_model = cb_models[best_model_idx]\n",
    "\n",
    "print(f\"Using CatBoost model from fold {best_model_idx + 1} (best F1: {cb_fold_scores[best_model_idx]:.4f})\\n\")\n",
    "\n",
    "# Calculate SHAP values\n",
    "print(\"Calculating SHAP values...\")\n",
    "explainer = shap.TreeExplainer(shap_model)\n",
    "shap_values = explainer.shap_values(X_sample)\n",
    "\n",
    "# shap_values is a list: [shap_values_class_0, shap_values_class_1]\n",
    "shap_values_class1 = shap_values[1] if isinstance(shap_values, list) else shap_values\n",
    "\n",
    "print(f\"✅ SHAP values calculated: shape {shap_values_class1.shape}\\n\")\n",
    "\n",
    "# ============================================================\n",
    "# 1. GLOBAL FEATURE IMPORTANCE (SHAP)\n",
    "# ============================================================\n",
    "print(\"=\"*70)\n",
    "print(\"1. GLOBAL FEATURE IMPORTANCE (SHAP)\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Mean absolute SHAP values = global importance\n",
    "shap_importance = np.abs(shap_values_class1).mean(axis=0)\n",
    "shap_importance_df = pd.DataFrame({\n",
    "    'feature': feature_names[:len(shap_importance)],  # Ensure same length\n",
    "    'shap_importance': shap_importance,\n",
    "}).sort_values('shap_importance', ascending=False)\n",
    "\n",
    "print(\"\\nTop 30 Features by SHAP Importance:\")\n",
    "print(shap_importance_df.head(30).to_string(index=False))\n",
    "\n",
    "print(\"\\nBottom 20 Features by SHAP Importance (potentially useless):\")\n",
    "print(shap_importance_df.tail(20).to_string(index=False))\n",
    "\n",
    "# ============================================================\n",
    "# 2. FEATURE IMPACT BY CLASS\n",
    "# ============================================================\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"2. FEATURE IMPACT BY CLASS (TDE vs Non-TDE)\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Separate SHAP values by actual class\n",
    "tde_shap = shap_values_class1[y_sample == 1]\n",
    "nontde_shap = shap_values_class1[y_sample == 0]\n",
    "\n",
    "print(f\"\\nTDE samples: {len(tde_shap)}, Non-TDE samples: {len(nontde_shap)}\")\n",
    "\n",
    "tde_mean_shap = np.abs(tde_shap).mean(axis=0)\n",
    "nontde_mean_shap = np.abs(nontde_shap).mean(axis=0)\n",
    "\n",
    "class_impact_df = pd.DataFrame({\n",
    "    'feature': feature_names[:len(tde_mean_shap)],\n",
    "    'shap_TDE': tde_mean_shap,\n",
    "    'shap_NonTDE': nontde_mean_shap,\n",
    "    'shap_diff': tde_mean_shap - nontde_mean_shap,\n",
    "}).sort_values('shap_diff', ascending=False)\n",
    "\n",
    "print(\"\\nTop 20 Features MORE Important for TDE predictions:\")\n",
    "print(class_impact_df.head(20)[['feature', 'shap_TDE', 'shap_NonTDE', 'shap_diff']].to_string(index=False))\n",
    "\n",
    "print(\"\\nTop 20 Features MORE Important for Non-TDE predictions:\")\n",
    "print(class_impact_df.tail(20)[['feature', 'shap_TDE', 'shap_NonTDE', 'shap_diff']].to_string(index=False))\n",
    "\n",
    "# Features with very different importance between classes\n",
    "class_specific = class_impact_df[np.abs(class_impact_df['shap_diff']) > 0.01].sort_values('shap_diff', key=abs, ascending=False)\n",
    "print(f\"\\n   Found {len(class_specific)} features with CLASS-SPECIFIC importance:\")\n",
    "print(class_specific.head(15)[['feature', 'shap_TDE', 'shap_NonTDE', 'shap_diff']].to_string(index=False))\n",
    "print(f\"   → These features behave differently for TDE vs Non-TDE\")\n",
    "\n",
    "# ============================================================\n",
    "# 3. MISCLASSIFICATION ANALYSIS WITH SHAP\n",
    "# ============================================================\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"3. MISCLASSIFICATION ANALYSIS WITH SHAP\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Get predictions for sample\n",
    "sample_preds = shap_model.predict_proba(X_sample)[:, 1]\n",
    "sample_preds_binary = (sample_preds >= cb_fold_thresholds[best_model_idx]).astype(int)\n",
    "sample_y = y_sample\n",
    "\n",
    "# Find misclassifications\n",
    "fp_mask = (sample_preds_binary == 1) & (sample_y == 0)\n",
    "fn_mask = (sample_preds_binary == 0) & (sample_y == 1)\n",
    "tp_mask = (sample_preds_binary == 1) & (sample_y == 1)\n",
    "tn_mask = (sample_preds_binary == 0) & (sample_y == 0)\n",
    "\n",
    "print(f\"\\nFP (False Positives): {fp_mask.sum()}\")\n",
    "print(f\"FN (False Negatives): {fn_mask.sum()}\")\n",
    "print(f\"TP (True Positives): {tp_mask.sum()}\")\n",
    "print(f\"TN (True Negatives): {tn_mask.sum()}\")\n",
    "\n",
    "if fp_mask.sum() > 0:\n",
    "    fp_shap = shap_values_class1[fp_mask]\n",
    "    tn_shap = shap_values_class1[tn_mask][:min(fp_mask.sum(), tn_mask.sum())]\n",
    "    \n",
    "    fp_mean_shap = np.abs(fp_shap).mean(axis=0)\n",
    "    tn_mean_shap = np.abs(tn_shap).mean(axis=0)\n",
    "    \n",
    "    fp_analysis = pd.DataFrame({\n",
    "        'feature': feature_names[:len(fp_mean_shap)],\n",
    "        'fp_shap': fp_mean_shap,\n",
    "        'tn_shap': tn_mean_shap,\n",
    "        'diff': fp_mean_shap - tn_mean_shap,\n",
    "    }).sort_values('diff', ascending=False)\n",
    "    \n",
    "    print(\"\\n   Top 15 Features causing False Positives (confusing Non-TDE as TDE):\")\n",
    "    print(fp_analysis.head(15)[['feature', 'fp_shap', 'tn_shap', 'diff']].to_string(index=False))\n",
    "\n",
    "if fn_mask.sum() > 0:\n",
    "    fn_shap = shap_values_class1[fn_mask]\n",
    "    tp_shap = shap_values_class1[tp_mask][:min(fn_mask.sum(), tp_mask.sum())]\n",
    "    \n",
    "    fn_mean_shap = np.abs(fn_shap).mean(axis=0)\n",
    "    tp_mean_shap = np.abs(tp_shap).mean(axis=0)\n",
    "    \n",
    "    fn_analysis = pd.DataFrame({\n",
    "        'feature': feature_names[:len(fn_mean_shap)],\n",
    "        'fn_shap': fn_mean_shap,\n",
    "        'tp_shap': tp_mean_shap,\n",
    "        'diff': fn_mean_shap - tp_mean_shap,\n",
    "    }).sort_values('diff', ascending=False)\n",
    "    \n",
    "    print(\"\\n   Top 15 Features causing False Negatives (missing TDE):\")\n",
    "    print(fn_analysis.head(15)[['feature', 'fn_shap', 'tp_shap', 'diff']].to_string(index=False))\n",
    "\n",
    "# ============================================================\n",
    "# 4. FEATURE VALUES THAT CONFUSE MODEL\n",
    "# ============================================================\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"4. FEATURE VALUES THAT CAUSE CONFUSION\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# For misclassified samples, check if SHAP values are unusually high/low\n",
    "if fp_mask.sum() > 0 or fn_mask.sum() > 0:\n",
    "    misclassified_shap = shap_values_class1[fp_mask | fn_mask]\n",
    "    correctly_classified_shap = shap_values_class1[tp_mask | tn_mask]\n",
    "    \n",
    "    if len(misclassified_shap) > 0 and len(correctly_classified_shap) > 0:\n",
    "        misclassified_mean = misclassified_shap.mean(axis=0)\n",
    "        correct_mean = correctly_classified_shap.mean(axis=0)\n",
    "        \n",
    "        confusion_df = pd.DataFrame({\n",
    "            'feature': feature_names[:len(misclassified_mean)],\n",
    "            'misclassified_shap': misclassified_mean,\n",
    "            'correct_shap': correct_mean,\n",
    "            'shap_diff': misclassified_mean - correct_mean,\n",
    "        }).sort_values('shap_diff', key=abs, ascending=False)\n",
    "        \n",
    "        print(\"\\n   Features with DIFFERENT SHAP values for misclassified vs correctly classified:\")\n",
    "        print(confusion_df.head(20)[['feature', 'misclassified_shap', 'correct_shap', 'shap_diff']].to_string(index=False))\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"KEY INSIGHTS FROM SHAP\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "insights = []\n",
    "\n",
    "if len(class_specific) > 10:\n",
    "    insights.append(f\"⚠️  {len(class_specific)} features have class-specific importance\")\n",
    "    insights.append(\"   → Consider class-specific feature engineering or weighted features\")\n",
    "\n",
    "if fp_mask.sum() > 5 or fn_mask.sum() > 5:\n",
    "    insights.append(f\"⚠️  Misclassifications found: {fp_mask.sum()} FP, {fn_mask.sum()} FN\")\n",
    "    insights.append(\"   → Check feature values and interactions for these cases\")\n",
    "\n",
    "if len(insights) == 0:\n",
    "    print(\"\\n✅ SHAP analysis shows consistent feature importance\")\n",
    "else:\n",
    "    print(\"\\n\")\n",
    "    for i, insight in enumerate(insights, 1):\n",
    "        print(f\"{i}. {insight}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "\n",
    "# Save SHAP importance for later use\n",
    "shap_importance_df.to_csv(f\"{GBM_MODEL_DIR}/shap_importance.csv\", index=False)\n",
    "print(f\"\\n✅ Saved SHAP importance to {GBM_MODEL_DIR}/shap_importance.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-21T16:50:52.506364Z",
     "iopub.status.busy": "2025-12-21T16:50:52.506013Z",
     "iopub.status.idle": "2025-12-21T16:50:52.679605Z",
     "shell.execute_reply": "2025-12-21T16:50:52.678538Z",
     "shell.execute_reply.started": "2025-12-21T16:50:52.506311Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "SHAP-BASED FEATURE OPTIMIZATION\n",
      "======================================================================\n",
      "\n",
      "Total features (after SelectKBest): 246\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "STEP 1: REMOVE USELESS FEATURES\n",
      "----------------------------------------------------------------------\n",
      "Features to remove (SHAP < 0.001): 24\n",
      "  Examples: ['y_flux_min', 'gp2d_peak_dt_r_z', 'i_gp2d_time_bwd_0.5', 'i_gp2d_time_bwd_0.8', 'snr_median', 'r_gp2d_peaks_pos_count', 'i_gp2d_negative_width', 'i_gp2d_rise_decline_ratio', 'y_flux_mean', 'u_gp2d_fwhm']\n",
      "\n",
      "✅ Removed 24 useless features\n",
      "   Remaining features: 284\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "STEP 2: CREATE FEATURE INTERACTIONS FOR TOP FEATURES\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "Top 10 SHAP features:\n",
      "  1. i_gp2d_peaks_pos_frac_2: 0.4886\n",
      "  2. i_gp2d_time_fwd_0.5: 0.4202\n",
      "  3. i_snr_max: 0.3551\n",
      "  4. i_gp2d_decline_rate: 0.3127\n",
      "  5. flux_std_all: 0.2035\n",
      "  6. flux_range_all: 0.1878\n",
      "  7. flux_max_all: 0.1817\n",
      "  8. r_gp2d_time_bwd_0.8: 0.1675\n",
      "  9. count_max_fall_30: 0.1592\n",
      "  10. r_flux_std: 0.1575\n",
      "\n",
      "Creating interactions for top i-band features...\n",
      "✅ Created 9 feature interactions\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "STEP 3: CREATE CLASS-SPECIFIC FEATURES\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "Creating weighted features for TDE-important features...\n",
      "✅ Created 4 class-specific features\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "STEP 4: CREATE ANTI-FP FEATURES\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "Creating anti-FP features (to reduce false positives)...\n",
      "✅ Created 4 anti-FP features\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "STEP 5: CREATE ANTI-FN FEATURES\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "Creating anti-FN features (to reduce false negatives)...\n",
      "✅ Created 3 anti-FN features\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "STEP 6: FINAL CLEANUP\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "Final feature count: 304\n",
      "  - Original: 308\n",
      "  - Removed useless: 24\n",
      "  - Added (interactions + transforms): 20\n",
      "\n",
      "✅ Feature optimization completed!\n",
      "   Optimized shape: Train (3043, 304), Test (7135, 304)\n",
      "✅ Saved optimized feature names\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# CELL 19: SHAP-BASED FEATURE OPTIMIZATION\n",
    "# ============================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"SHAP-BASED FEATURE OPTIMIZATION\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Load SHAP importance if not in memory\n",
    "import os\n",
    "if 'shap_importance_df' not in locals():\n",
    "    if os.path.exists(f\"{GBM_MODEL_DIR}/shap_importance.csv\"):\n",
    "        shap_importance_df = pd.read_csv(f\"{GBM_MODEL_DIR}/shap_importance.csv\")\n",
    "        print(\"✅ Loaded SHAP importance from file\")\n",
    "    else:\n",
    "        print(\"❌ SHAP importance not found. Run CELL 18 first!\")\n",
    "        raise FileNotFoundError(\"Please run CELL 18 (SHAP Analysis) first\")\n",
    "\n",
    "print(f\"\\nTotal features (after SelectKBest): {len(shap_importance_df)}\")\n",
    "\n",
    "# Get original feature data (before SelectKBest transformation)\n",
    "# We need to work with original X_df, X_test_df\n",
    "X_df_orig = train_feat.copy()\n",
    "X_test_df_orig = test_feat.copy()\n",
    "\n",
    "if \"object_id\" in X_df_orig.columns:\n",
    "    X_df_orig = X_df_orig.drop(columns=[\"object_id\"])\n",
    "if \"object_id\" in X_test_df_orig.columns:\n",
    "    X_test_df_orig = X_test_df_orig.drop(columns=[\"object_id\"])\n",
    "\n",
    "common_cols_orig = sorted(set(X_df_orig.columns) & set(X_test_df_orig.columns))\n",
    "X_df_orig = X_df_orig[common_cols_orig]\n",
    "X_test_df_orig = X_test_df_orig[common_cols_orig]\n",
    "\n",
    "X_df_orig = X_df_orig.replace([np.inf, -np.inf], np.nan).fillna(0)\n",
    "X_test_df_orig = X_test_df_orig.replace([np.inf, -np.inf], np.nan).fillna(0)\n",
    "\n",
    "# ============================================================\n",
    "# STEP 1: REMOVE USELESS FEATURES (SHAP importance < threshold)\n",
    "# ============================================================\n",
    "print(\"\\n\" + \"-\"*70)\n",
    "print(\"STEP 1: REMOVE USELESS FEATURES\")\n",
    "print(\"-\"*70)\n",
    "\n",
    "# Threshold: remove features with SHAP importance < 0.001\n",
    "SHAP_THRESHOLD = 0.001\n",
    "\n",
    "useless_features = shap_importance_df[\n",
    "    shap_importance_df['shap_importance'] < SHAP_THRESHOLD\n",
    "]['feature'].tolist()\n",
    "\n",
    "print(f\"Features to remove (SHAP < {SHAP_THRESHOLD}): {len(useless_features)}\")\n",
    "if len(useless_features) > 0:\n",
    "    print(f\"  Examples: {useless_features[:10]}\")\n",
    "\n",
    "# Remove from original datasets\n",
    "X_df_cleaned = X_df_orig.drop(columns=[f for f in useless_features if f in X_df_orig.columns], errors='ignore')\n",
    "X_test_df_cleaned = X_test_df_orig.drop(columns=[f for f in useless_features if f in X_test_df_orig.columns], errors='ignore')\n",
    "\n",
    "print(f\"\\n✅ Removed {len(X_df_orig.columns) - len(X_df_cleaned.columns)} useless features\")\n",
    "print(f\"   Remaining features: {len(X_df_cleaned.columns)}\")\n",
    "\n",
    "# ============================================================\n",
    "# STEP 2: CREATE FEATURE INTERACTIONS FOR TOP FEATURES\n",
    "# ============================================================\n",
    "print(\"\\n\" + \"-\"*70)\n",
    "print(\"STEP 2: CREATE FEATURE INTERACTIONS FOR TOP FEATURES\")\n",
    "print(\"-\"*70)\n",
    "\n",
    "# Get top 10 features by SHAP importance\n",
    "top_shap_features = shap_importance_df.head(10)['feature'].tolist()\n",
    "print(f\"\\nTop 10 SHAP features:\")\n",
    "for i, feat in enumerate(top_shap_features, 1):\n",
    "    shap_val = shap_importance_df[shap_importance_df['feature'] == feat]['shap_importance'].iloc[0]\n",
    "    print(f\"  {i}. {feat}: {shap_val:.4f}\")\n",
    "\n",
    "# Create interactions between top features (multiplicative)\n",
    "print(f\"\\nCreating interactions for top i-band features...\")\n",
    "interaction_count = 0\n",
    "\n",
    "# Focus on i-band features (they're most important)\n",
    "i_band_features = [f for f in top_shap_features if f.startswith('i_')]\n",
    "other_top_features = [f for f in top_shap_features if not f.startswith('i_')]\n",
    "\n",
    "# Create interactions: top i-feature × top other feature\n",
    "for i_feat in i_band_features[:3]:  # Top 3 i-features\n",
    "    if i_feat not in X_df_cleaned.columns:\n",
    "        continue\n",
    "    \n",
    "    for other_feat in other_top_features[:3]:  # Top 3 other features\n",
    "        if other_feat not in X_df_cleaned.columns:\n",
    "            continue\n",
    "        \n",
    "        # Skip self-interaction\n",
    "        if i_feat == other_feat:\n",
    "            continue\n",
    "        \n",
    "        # Create multiplicative interaction\n",
    "        interaction_name = f\"{i_feat}_x_{other_feat}\"\n",
    "        \n",
    "        # Normalize before multiplication to avoid extreme values\n",
    "        i_norm = (X_df_cleaned[i_feat] - X_df_cleaned[i_feat].mean()) / (X_df_cleaned[i_feat].std() + 1e-10)\n",
    "        other_norm = (X_df_cleaned[other_feat] - X_df_cleaned[other_feat].mean()) / (X_df_cleaned[other_feat].std() + 1e-10)\n",
    "        \n",
    "        X_df_cleaned[interaction_name] = i_norm * other_norm\n",
    "        X_test_df_cleaned[interaction_name] = (\n",
    "            (X_test_df_cleaned[i_feat] - X_df_cleaned[i_feat].mean()) / (X_df_cleaned[i_feat].std() + 1e-10) *\n",
    "            (X_test_df_cleaned[other_feat] - X_df_cleaned[other_feat].mean()) / (X_df_cleaned[other_feat].std() + 1e-10)\n",
    "        )\n",
    "        interaction_count += 1\n",
    "\n",
    "print(f\"✅ Created {interaction_count} feature interactions\")\n",
    "\n",
    "# ============================================================\n",
    "# STEP 3: CREATE CLASS-SPECIFIC FEATURES (for TDE)\n",
    "# ============================================================\n",
    "print(\"\\n\" + \"-\"*70)\n",
    "print(\"STEP 3: CREATE CLASS-SPECIFIC FEATURES\")\n",
    "print(\"-\"*70)\n",
    "\n",
    "# Features important for TDE predictions (from SHAP analysis)\n",
    "tde_important_features = [\n",
    "    'i_gp2d_decline_rate',\n",
    "    'flux_range_all',\n",
    "    'i_snr_max',\n",
    "    'i_gp2d_time_fwd_0.5',\n",
    "]\n",
    "\n",
    "print(f\"\\nCreating weighted features for TDE-important features...\")\n",
    "class_specific_count = 0\n",
    "\n",
    "for feat in tde_important_features:\n",
    "    if feat not in X_df_cleaned.columns:\n",
    "        continue\n",
    "    \n",
    "    # Create squared feature (non-linear transformation)\n",
    "    X_df_cleaned[f\"{feat}_squared\"] = X_df_cleaned[feat] ** 2\n",
    "    X_test_df_cleaned[f\"{feat}_squared\"] = X_test_df_cleaned[feat] ** 2\n",
    "    \n",
    "    # Create log feature if positive\n",
    "    if (X_df_cleaned[feat] > 0).all():\n",
    "        X_df_cleaned[f\"{feat}_log\"] = np.log1p(X_df_cleaned[feat])\n",
    "        X_test_df_cleaned[f\"{feat}_log\"] = np.log1p(X_test_df_cleaned[feat])\n",
    "        class_specific_count += 1\n",
    "    \n",
    "    class_specific_count += 1\n",
    "\n",
    "print(f\"✅ Created {class_specific_count} class-specific features\")\n",
    "\n",
    "# ============================================================\n",
    "# STEP 4: CREATE ANTI-FP FEATURES (to reduce False Positives)\n",
    "# ============================================================\n",
    "print(\"\\n\" + \"-\"*70)\n",
    "print(\"STEP 4: CREATE ANTI-FP FEATURES\")\n",
    "print(\"-\"*70)\n",
    "\n",
    "# Features that cause False Positives (from SHAP analysis)\n",
    "fp_features = [\n",
    "    'i_gp2d_time_fwd_0.5',\n",
    "    'i_gp2d_decline_rate',\n",
    "    'flux_range_all',\n",
    "    'u_gp2d_abs_diff',\n",
    "]\n",
    "\n",
    "print(f\"\\nCreating anti-FP features (to reduce false positives)...\")\n",
    "anti_fp_count = 0\n",
    "\n",
    "for feat in fp_features:\n",
    "    if feat not in X_df_cleaned.columns:\n",
    "        continue\n",
    "    \n",
    "    # Create threshold-based feature: is feature value > median?\n",
    "    median_val = X_df_cleaned[feat].median()\n",
    "    X_df_cleaned[f\"{feat}_high\"] = (X_df_cleaned[feat] > median_val).astype(float)\n",
    "    X_test_df_cleaned[f\"{feat}_high\"] = (X_test_df_cleaned[feat] > median_val).astype(float)\n",
    "    \n",
    "    anti_fp_count += 1\n",
    "\n",
    "print(f\"✅ Created {anti_fp_count} anti-FP features\")\n",
    "\n",
    "# ============================================================\n",
    "# STEP 5: CREATE ANTI-FN FEATURES (to reduce False Negatives)\n",
    "# ============================================================\n",
    "print(\"\\n\" + \"-\"*70)\n",
    "print(\"STEP 5: CREATE ANTI-FN FEATURES\")\n",
    "print(\"-\"*70)\n",
    "\n",
    "# Features important for FN cases (from SHAP analysis)\n",
    "fn_features = [\n",
    "    'u_gp2d_abs_diff',\n",
    "    'i_snr_mean',\n",
    "    'Z',\n",
    "]\n",
    "\n",
    "print(f\"\\nCreating anti-FN features (to reduce false negatives)...\")\n",
    "anti_fn_count = 0\n",
    "\n",
    "for feat in fn_features:\n",
    "    if feat not in X_df_cleaned.columns:\n",
    "        continue\n",
    "    \n",
    "    # Create threshold-based feature\n",
    "    median_val = X_df_cleaned[feat].median()\n",
    "    X_df_cleaned[f\"{feat}_low\"] = (X_df_cleaned[feat] < median_val).astype(float)\n",
    "    X_test_df_cleaned[f\"{feat}_low\"] = (X_test_df_cleaned[feat] < median_val).astype(float)\n",
    "    \n",
    "    anti_fn_count += 1\n",
    "\n",
    "print(f\"✅ Created {anti_fn_count} anti-FN features\")\n",
    "\n",
    "# ============================================================\n",
    "# STEP 6: FINAL CLEANUP\n",
    "# ============================================================\n",
    "print(\"\\n\" + \"-\"*70)\n",
    "print(\"STEP 6: FINAL CLEANUP\")\n",
    "print(\"-\"*70)\n",
    "\n",
    "# Ensure same columns\n",
    "common_cols = sorted(set(X_df_cleaned.columns) & set(X_test_df_cleaned.columns))\n",
    "X_df_cleaned = X_df_cleaned[common_cols]\n",
    "X_test_df_cleaned = X_test_df_cleaned[common_cols]\n",
    "\n",
    "print(f\"\\nFinal feature count: {len(common_cols)}\")\n",
    "print(f\"  - Original: {len(X_df_orig.columns)}\")\n",
    "print(f\"  - Removed useless: {len(useless_features)}\")\n",
    "print(f\"  - Added (interactions + transforms): {len(common_cols) - (len(X_df_orig.columns) - len([f for f in useless_features if f in X_df_orig.columns]))}\")\n",
    "\n",
    "# Replace NaN/Inf\n",
    "X_df_cleaned = X_df_cleaned.replace([np.inf, -np.inf], np.nan).fillna(0)\n",
    "X_test_df_cleaned = X_test_df_cleaned.replace([np.inf, -np.inf], np.nan).fillna(0)\n",
    "\n",
    "# Update feature names and data\n",
    "feature_names_optimized = X_df_cleaned.columns.tolist()\n",
    "X_raw_optimized = X_df_cleaned.values\n",
    "X_test_raw_optimized = X_test_df_cleaned.values\n",
    "\n",
    "print(f\"\\n✅ Feature optimization completed!\")\n",
    "print(f\"   Optimized shape: Train {X_raw_optimized.shape}, Test {X_test_raw_optimized.shape}\")\n",
    "\n",
    "# Save optimized feature names\n",
    "joblib.dump(feature_names_optimized, f\"{GBM_MODEL_DIR}/optimized_feature_names.pkl\")\n",
    "print(f\"✅ Saved optimized feature names\")"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 8729261,
     "sourceId": 13720337,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 8964107,
     "sourceId": 14080758,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31234,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": ".venv (3.12.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
